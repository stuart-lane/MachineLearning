{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQlWJ1f2jMV7xiwVB/lZfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stuart-lane/MachineLearning/blob/main/LogitSimulations2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CDXNFhVBDeUO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "np.random.seed(123456)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATING PROCESS"
      ],
      "metadata": {
        "id": "uzVdX-gmuS9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "x = np.random.randn(n)\n",
        "u = np.random.logistic(0, 1, n)\n",
        "beta0 = 0\n",
        "beta1 = 1\n",
        "\n",
        "ystar = beta0 + beta1 * x + u\n",
        "y = (ystar > 0).astype(int)\n",
        "\n",
        "#p = 1 / (1 + np.exp(-(beta0 + x*beta1 + u)))\n",
        "#y = np.random.binomial(n = 1, p = p, size = n)"
      ],
      "metadata": {
        "id": "qVd8us9nDqP_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 123)\n",
        "\n",
        "train_df_keys = {'constant': x_train[:,0].flatten(), 'X1': x_train[:,1].flatten(), 'y': y_train.flatten()}\n",
        "train_df = pd.DataFrame(train_df_keys)"
      ],
      "metadata": {
        "id": "4PMPLz32Ff0e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION WITH SKLEARN"
      ],
      "metadata": {
        "id": "wFPUh81huciJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "0c4MyJhDFckl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(solver = 'lbfgs', max_iter = 1000)  # You can change the solver and max_iter as needed\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "f7uIPvpvpqgs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "dfb7cd68-ca88-4982-a5ad-2eb85add03a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intercept = model.intercept_[0]\n",
        "coefficient = model.coef_[0][1]"
      ],
      "metadata": {
        "id": "QPqhlggip0eM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Intercept: {intercept}\")\n",
        "print(f\"Coefficient for 'Regressor': {coefficient}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE2ZTPE1p4DG",
        "outputId": "30463c06-c62b-4c00-a973-b3af89d5febb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: -0.12587614423279017\n",
            "Coefficient for 'Regressor': 0.9506309771391525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STANDARD LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "S0A-pPjivQEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression(X, y, learning_rate, num_iterations):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        theta -= learning_rate * gradient\n",
        "        if (i+1) % 100 == 0:\n",
        "          print(f\"Iteration: {i+1} || Gradients: {gradient[0][0]} and {gradient[1][0]}\")\n",
        "          print(f\"Estimated Parameters (Weights): {theta[0][0]} and {theta[1][0]}\")\n",
        "\n",
        "    return theta\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_iterations = 1000\n",
        "\n",
        "theta = logistic_regression(x_train, y_train, learning_rate, num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqijV77vu_eE",
        "outputId": "d26bf9bd-1c2c-4b7e-daeb-5d5738b555c9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100 || Gradients: 0.0024484652999759928 and -0.024826645218115048\n",
            "Estimated Parameters (Weights): -0.11329131281618507 and 0.81057807398994\n",
            "Iteration: 200 || Gradients: 0.0003652894496281411 and -0.00469650720890295\n",
            "Estimated Parameters (Weights): -0.12381547601229506 and 0.9287924239136948\n",
            "Iteration: 300 || Gradients: 6.741809573183612e-05 and -0.0009668692203220522\n",
            "Estimated Parameters (Weights): -0.1255479423812796 and 0.9521417298211473\n",
            "Iteration: 400 || Gradients: 1.3323826691407498e-05 and -0.00020236601955596283\n",
            "Estimated Parameters (Weights): -0.1258777871830009 and 0.9569891455894065\n",
            "Iteration: 500 || Gradients: 2.7082526901361924e-06 and -4.249818941969404e-05\n",
            "Estimated Parameters (Weights): -0.12594378392045522 and 0.9580054499044263\n",
            "Iteration: 600 || Gradients: 5.583272185494259e-07 and -8.93091069069759e-06\n",
            "Estimated Parameters (Weights): -0.1259572801211017 and 0.9582189541023278\n",
            "Iteration: 700 || Gradients: 1.1601941852526498e-07 and -1.8770430608018932e-06\n",
            "Estimated Parameters (Weights): -0.12596007182452745 and 0.9582638244779405\n",
            "Iteration: 800 || Gradients: 2.4220607813957296e-08 and -3.9451091832752583e-07\n",
            "Estimated Parameters (Weights): -0.12596065307192877 and 0.9582732551362777\n",
            "Iteration: 900 || Gradients: 5.0702631237964365e-09 and -8.291674903584956e-08\n",
            "Estimated Parameters (Weights): -0.1259607745556874 and 0.9582752372400498\n",
            "Iteration: 1000 || Gradients: 1.063120431271658e-09 and -1.7427034027586717e-08\n",
            "Estimated Parameters (Weights): -0.12596080000417947 and 0.9582756538300314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUTOGRAD"
      ],
      "metadata": {
        "id": "M_uavi5mwj0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.optim import LBFGS"
      ],
      "metadata": {
        "id": "rkjzhx7tw1dg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_keys = {'constant': x_train[:,0].flatten(), 'X1': x_train[:,1].flatten(), 'y': y_train.flatten()}\n",
        "\n",
        "train_df = pd.DataFrame(train_df_keys)"
      ],
      "metadata": {
        "id": "hABpWk9twl8k"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.cat((torch.tensor(train_df[[\"constant\", \"X1\"]].values, dtype=torch.float64),), dim=1)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(train_df[\"y\"].values, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.view(-1, 1)\n",
        "\n",
        "beta_initial = np.array([0, 0])\n",
        "beta = torch.zeros(2, 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "\n",
        "print(f\"Running on: {dev}\")\n",
        "\n",
        "# Define the loss function\n",
        "def ll(beta):\n",
        "    xbetahat = torch.mm(x, beta)\n",
        "    yhat = torch.sigmoid(xbetahat)\n",
        "    loss = - torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    return loss\n",
        "\n",
        "# Set up the Adam optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = LBFGS([beta], lr = learning_rate)\n",
        "\n",
        "# Define a function to calculate the loss and update model parameters\n",
        "def calculate_loss():\n",
        "    optimizer.zero_grad()\n",
        "    value = ll(beta)\n",
        "    value.backward()\n",
        "    return value\n",
        "\n",
        "num_iter = 100\n",
        "for i in range(1, num_iter + 1):\n",
        "  logl = optimizer.step(calculate_loss)\n",
        "  logl_value = logl.detach().item()\n",
        "  estimated_weights = beta.detach().numpy()\n",
        "  if (i + 1) % 10 == 0:\n",
        "    print(f\"Iteration {i + 1} || Log-value {logl_value}\")\n",
        "    print(f\"Estimated weights: {estimated_weights}\")\n",
        "\n",
        "estimated_weights_LBFGS = beta.detach().numpy()\n",
        "print(\"Estimated Parameters (Weights):\", estimated_weights_LBFGS)"
      ],
      "metadata": {
        "id": "uSqgwPQ0yWxY",
        "outputId": "d7520406-6e33-45dc-9981-362c73a051ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n",
            "Iteration 10 || Log-value 478.8465763149336\n",
            "Estimated weights: [[-0.12596   ]\n",
            " [ 0.95826996]]\n",
            "Iteration 20 || Log-value 478.8465763127937\n",
            "Estimated weights: [[-0.12596042]\n",
            " [ 0.95827299]]\n",
            "Iteration 30 || Log-value 478.8465763127937\n",
            "Estimated weights: [[-0.12596042]\n",
            " [ 0.95827299]]\n",
            "Iteration 40 || Log-value 478.8465763127937\n",
            "Estimated weights: [[-0.12596042]\n",
            " [ 0.95827299]]\n",
            "Iteration 50 || Log-value 478.8465763127937\n",
            "Estimated weights: [[-0.12596042]\n",
            " [ 0.95827299]]\n",
            "Iteration 60 || Log-value 478.8465763127937\n",
            "Estimated weights: [[-0.12596042]\n",
            " [ 0.95827299]]\n",
            "Iteration 70 || Log-value 478.8465763127937\n",
            "Estimated weights: [[-0.12596042]\n",
            " [ 0.95827299]]\n",
            "Iteration 80 || Log-value 478.8465763127937\n",
            "Estimated weights: [[-0.12596042]\n",
            " [ 0.95827299]]\n",
            "Iteration 90 || Log-value 478.8465763127937\n",
            "Estimated weights: [[-0.12596042]\n",
            " [ 0.95827299]]\n",
            "Iteration 100 || Log-value 478.8465763127937\n",
            "Estimated weights: [[-0.12596042]\n",
            " [ 0.95827299]]\n",
            "Estimated Parameters (Weights): [[-0.12596042]\n",
            " [ 0.95827299]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEURAL NETWORK LOGIT"
      ],
      "metadata": {
        "id": "9XOZy2a5Ojma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "cBhQa6x3OlwS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogitModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogitModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # Single output unit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.linear.weight.dtype)  # Cast input to the same data type as the model's parameters\n",
        "        x = self.linear(x)\n",
        "        x = torch.sigmoid(x)  # Apply sigmoid activation for binary classification\n",
        "        return x\n",
        "\n",
        "x = torch.tensor(x_train, dtype = torch.float64)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(y_train, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "input_dim = x.shape[1]\n",
        "\n",
        "model = LogitModel(input_dim)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.1)\n",
        "\n",
        "epochs = 5000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "    outputs = model(x)\n",
        "    outputs = outputs.float()\n",
        "    y = y.float()\n",
        "\n",
        "    loss = loss_function(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % (epochs / 10) == 0:\n",
        "        print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# View the learned parameters\n",
        "print(\"Learned parameters:\")\n",
        "print(f\"Model weights: {model.linear.weight[0][0]}, {model.linear.weight[0][1]}\") # these are the parameters of interest\n",
        "print(f\"Model bias:{model.linear.bias[0]}\")"
      ],
      "metadata": {
        "id": "Rs6icLu8ijPF",
        "outputId": "9b22ffbe-6a45-4df4-cc16-42e0bfdfa933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/5000], Loss: 0.7002\n",
            "Epoch [500/5000], Loss: 0.5986\n",
            "Epoch [1000/5000], Loss: 0.5986\n",
            "Epoch [1500/5000], Loss: 0.5986\n",
            "Epoch [2000/5000], Loss: 0.5986\n",
            "Epoch [2500/5000], Loss: 0.5986\n",
            "Epoch [3000/5000], Loss: 0.5986\n",
            "Epoch [3500/5000], Loss: 0.5986\n",
            "Epoch [4000/5000], Loss: 0.5986\n",
            "Epoch [4500/5000], Loss: 0.5986\n",
            "Learned parameters:\n",
            "Model weights: -0.15777356922626495, 0.9582759141921997\n",
            "Model bias:0.031812842935323715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORKING PYTORCH LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "jWDJw7Ja_FuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(x_train, dtype = torch.float64)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(y_train, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Initialize parameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 10\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "# Training loop\n",
        "for i in range(num_iterations):\n",
        "    xbetahat = torch.matmul(x, beta)\n",
        "    yhat = sigmoid(xbetahat)\n",
        "\n",
        "    loss = -torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        beta -= learning_rate * beta.grad\n",
        "    beta.grad.zero_()\n",
        "\n",
        "    print(f'Iteration {i+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Get the learned coefficients\n",
        "learned_beta = beta.detach().numpy()\n",
        "\n",
        "print(f'Learned Logistic Regression Coefficients (Beta): {learned_beta}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fshQKecVEx2t",
        "outputId": "94452de1-5cf7-4270-fd5e-c34ff89d285f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Loss: 839.2450018031193\n",
            "Iteration 2, Loss: 599.1861353337963\n",
            "Iteration 3, Loss: 492.3846833469495\n",
            "Iteration 4, Loss: 479.2939064387473\n",
            "Iteration 5, Loss: 479.0372641955145\n",
            "Iteration 6, Loss: 478.92708631838406\n",
            "Iteration 7, Loss: 478.8807817587528\n",
            "Iteration 8, Loss: 478.86109068399554\n",
            "Iteration 9, Loss: 478.85274167867703\n",
            "Iteration 10, Loss: 478.8491941862245\n",
            "Learned Logistic Regression Coefficients (Beta): [[-0.12229851]\n",
            " [ 0.9585038 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05xe8X9f72k3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}