{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSvd9aqnRKrMOTZYRpkRAF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stuart-lane/MachineLearning/blob/main/LogitSimulations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "CDXNFhVBDeUO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "np.random.seed(123456)\n",
        "#np.random.seed(7542716)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATING PROCESS"
      ],
      "metadata": {
        "id": "uzVdX-gmuS9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "x = np.random.randn(n)\n",
        "u = np.random.logistic(0, 1, n)\n",
        "beta0 = 0\n",
        "beta1 = 1\n",
        "\n",
        "ystar = beta0 + beta1 * x + u\n",
        "y = (ystar > 0).astype(int)\n",
        "\n",
        "#p = 1 / (1 + np.exp(-(beta0 + x*beta1 + u)))\n",
        "#y = np.random.binomial(n = 1, p = p, size = n)"
      ],
      "metadata": {
        "id": "qVd8us9nDqP_"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 123)"
      ],
      "metadata": {
        "id": "4PMPLz32Ff0e"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION WITH SKLEARN"
      ],
      "metadata": {
        "id": "wFPUh81huciJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "0c4MyJhDFckl"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(solver = 'lbfgs', max_iter = 1000)  # You can change the solver and max_iter as needed\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "f7uIPvpvpqgs",
        "outputId": "2be0398b-903f-4abd-cca3-d6ab407acfc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intercept = model.intercept_[0]\n",
        "coefficient = model.coef_[0][1]"
      ],
      "metadata": {
        "id": "QPqhlggip0eM"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Intercept: {intercept}\")\n",
        "print(f\"Coefficient for 'Regressor': {coefficient}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE2ZTPE1p4DG",
        "outputId": "5fe49d7e-535d-4aa3-f8ab-7aed3b1f6c89"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: -0.060131908967700765\n",
            "Coefficient for 'Regressor': 1.253262836265797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STANDARD LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "S0A-pPjivQEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the logistic function\n",
        "def logistic(x, beta):\n",
        "    x_beta = np.dot(x, beta)\n",
        "    x_beta = np.clip(x_beta, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x_beta))\n",
        "\n",
        "# Define the log-likelihood function\n",
        "def log_likelihood(beta, x, y):\n",
        "    p = logistic(x, beta)\n",
        "    epsilon = 1e-10\n",
        "    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "    ll = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "    return -ll\n",
        "\n",
        "def derivative(beta, x_train, y_train):\n",
        "    p = logistic(x_train, beta)\n",
        "    gradient = np.zeros(2)  # Initialize a 1D gradient array with size 2\n",
        "    gradient[0] = np.sum(y_train - p)\n",
        "    gradient[1] = np.sum(np.dot(x_train.T, y_train - p)) / x_train.shape[0]\n",
        "    return gradient\n",
        "\n",
        "beta_initial = np.array([0.5, 0.5])\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "result = minimize(log_likelihood, beta_initial, args=(x_train, y_train), method = 'L-BFGS-B', jac = derivative)\n",
        "\n",
        "print(result)\n",
        "\n",
        "estimated_beta = [f'{param:.7f}' for param in result.x]\n",
        "formatted_parameters = ', '.join(estimated_beta)\n",
        "\n",
        "print(f\"Estimated Parameters: [{formatted_parameters}]\")\n",
        "print(\"Negative Log-Likelihood:\", result.fun)"
      ],
      "metadata": {
        "id": "K7XUJgrd6qJH",
        "outputId": "7758153a-79f8-4f65-e3c2-87a39d3ee47f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  message: ABNORMAL_TERMINATION_IN_LNSRCH\n",
            "  success: False\n",
            "   status: 2\n",
            "      fun: 491889.4423803657\n",
            "        x: [ 5.000e-01  5.000e-01]\n",
            "      nit: 0\n",
            "      jac: [-9.307e+04  5.957e+01]\n",
            "     nfev: 20\n",
            "     njev: 20\n",
            " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
            "Estimated Parameters: [0.5000000, 0.5000000]\n",
            "Negative Log-Likelihood: 491889.4423803657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the logistic function\n",
        "def logistic(x, beta):\n",
        "    x_beta = np.dot(x, beta)\n",
        "    x_beta = np.clip(x_beta, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x_beta))\n",
        "\n",
        "# Define the log-likelihood function\n",
        "def log_likelihood(beta, x, y):\n",
        "    p = logistic(x, beta)\n",
        "    epsilon = 1e-10\n",
        "    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "    ll = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "    return -ll\n",
        "\n",
        "# Define the gradient (Jacobian) of the log-likelihood function\n",
        "def gradient(beta, x, y):\n",
        "    p = logistic(x, beta)\n",
        "    gradient_vector = np.dot(x.T, (y - p)) / x.shape[0]\n",
        "    return -gradient_vector\n",
        "\n",
        "beta_initial = np.array([0, 1])\n",
        "\n",
        "# Example usage of minimize\n",
        "result = minimize(log_likelihood, beta_initial, args=(x_train, y_train), method='BFGS')\n",
        "\n",
        "print(result)\n",
        "\n",
        "estimated_beta = [f'{param:.7f}' for param in result.x]\n",
        "formatted_parameters = ', '.join(estimated_beta)\n",
        "\n",
        "print(f\"Estimated Parameters: [{formatted_parameters}]\")\n",
        "print(\"Negative Log-Likelihood:\", result.fun)"
      ],
      "metadata": {
        "id": "HwmA-Aeg0exH",
        "outputId": "1781e3a4-efc5-420b-919b-1c5ac3dbe964",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  message: Desired error not necessarily achieved due to precision loss.\n",
            "  success: False\n",
            "   status: 2\n",
            "      fun: 443516.1905556687\n",
            "        x: [-3.500e-02  1.115e-08]\n",
            "      nit: 8\n",
            "      jac: [ 0.000e+00  1.172e-02]\n",
            " hess_inv: [[ 3.128e-06  2.009e-06]\n",
            "            [ 2.009e-06  2.201e-06]]\n",
            "     nfev: 36\n",
            "     njev: 12\n",
            "Estimated Parameters: [-0.0350036, 0.0000000]\n",
            "Negative Log-Likelihood: 443516.1905556687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the logistic function\n",
        "#def logistic(x, beta):\n",
        "#    x_beta = np.dot(x, beta)\n",
        "#    x_beta = np.clip(x_beta, -500, 500)\n",
        "#    return 1 / (1 + np.exp(-x_beta))\n",
        "\n",
        "# Define the log-likelihood function\n",
        "#def log_likelihood(beta, x, y):\n",
        "#    x\n",
        "#    p = logistic(x, beta)\n",
        "#    epsilon = 1e-10\n",
        "#    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "#    ll = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "#    return -ll\n",
        "\n",
        "# Define the log-likelihood function\n",
        "def log_likelihood(beta, x, y):\n",
        "    x_beta = x @ beta\n",
        "    p = 1 / (1 + np.exp(-x_beta))\n",
        "    epsilon = 1e-10\n",
        "    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "    ll = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "    return -ll\n",
        "\n",
        "beta_initial = np.array([0, 1])\n",
        "\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "result = minimize(log_likelihood, beta_initial, args=(x_train, y_train), method = 'L-BFGS-B')\n",
        "\n",
        "print(result)\n",
        "\n",
        "estimated_beta = [f'{param:.7f}' for param in result.x]\n",
        "formatted_parameters = ', '.join(estimated_beta)\n",
        "\n",
        "print(f\"Estimated Parameters: [{formatted_parameters}]\")\n",
        "print(\"Negative Log-Likelihood:\", result.fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI-OOLF6Fbg9",
        "outputId": "dee960a4-63c9-4596-9c0e-12a78d323d05"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
            "  success: True\n",
            "   status: 0\n",
            "      fun: 441930.7189702924\n",
            "        x: [-1.453e-01  1.547e-05]\n",
            "      nit: 4\n",
            "      jac: [-3.783e-01  2.643e+00]\n",
            "     nfev: 15\n",
            "     njev: 5\n",
            " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
            "Estimated Parameters: [-0.1452562, 0.0000155]\n",
            "Negative Log-Likelihood: 441930.7189702924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression(X, y, learning_rate, num_iterations):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        theta -= learning_rate * gradient\n",
        "        if (i+1) % 100 == 0:\n",
        "          print(f\"Iteration: {i+1} || Gradients: {gradient[0][0]} and {gradient[1][0]}\")\n",
        "          print(f\"Estimated Parameters (Weights): {theta[0][0]} and {theta[1][0]}\")\n",
        "\n",
        "    return theta\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_iterations = 1000\n",
        "\n",
        "theta = logistic_regression(x_train, y_train, learning_rate, num_iterations)\n",
        "\n",
        "#print(\"Estimated Parameters (Weights):\", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqijV77vu_eE",
        "outputId": "4c6cd753-c5b6-4092-be46-e65753c93201"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100 || Gradients: 0.0024484652999759928 and -0.024826645218115048\n",
            "Estimated Parameters (Weights): -0.11329131281618507 and 0.81057807398994\n",
            "Iteration: 200 || Gradients: 0.0003652894496281411 and -0.00469650720890295\n",
            "Estimated Parameters (Weights): -0.12381547601229506 and 0.9287924239136948\n",
            "Iteration: 300 || Gradients: 6.741809573183612e-05 and -0.0009668692203220522\n",
            "Estimated Parameters (Weights): -0.1255479423812796 and 0.9521417298211473\n",
            "Iteration: 400 || Gradients: 1.3323826691407498e-05 and -0.00020236601955596283\n",
            "Estimated Parameters (Weights): -0.1258777871830009 and 0.9569891455894065\n",
            "Iteration: 500 || Gradients: 2.7082526901361924e-06 and -4.249818941969404e-05\n",
            "Estimated Parameters (Weights): -0.12594378392045522 and 0.9580054499044263\n",
            "Iteration: 600 || Gradients: 5.583272185494259e-07 and -8.93091069069759e-06\n",
            "Estimated Parameters (Weights): -0.1259572801211017 and 0.9582189541023278\n",
            "Iteration: 700 || Gradients: 1.1601941852526498e-07 and -1.8770430608018932e-06\n",
            "Estimated Parameters (Weights): -0.12596007182452745 and 0.9582638244779405\n",
            "Iteration: 800 || Gradients: 2.4220607813957296e-08 and -3.9451091832752583e-07\n",
            "Estimated Parameters (Weights): -0.12596065307192877 and 0.9582732551362777\n",
            "Iteration: 900 || Gradients: 5.0702631237964365e-09 and -8.291674903584956e-08\n",
            "Estimated Parameters (Weights): -0.1259607745556874 and 0.9582752372400498\n",
            "Iteration: 1000 || Gradients: 1.063120431271658e-09 and -1.7427034027586717e-08\n",
            "Estimated Parameters (Weights): -0.12596080000417947 and 0.9582756538300314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUTOGRAD"
      ],
      "metadata": {
        "id": "M_uavi5mwj0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.optim import LBFGS"
      ],
      "metadata": {
        "id": "rkjzhx7tw1dg"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_keys = {'constant': x_train[:,0].flatten(), 'X1': x_train[:,1].flatten(), 'y': y_train.flatten()}\n",
        "\n",
        "train_df = pd.DataFrame(train_df_keys)\n",
        "train_df.head(20)"
      ],
      "metadata": {
        "id": "hABpWk9twl8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.cat((torch.tensor(train_df[[\"constant\", \"X1\"]].values, dtype=torch.float64),), dim=1)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(train_df[\"y\"].values, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.view(-1, 1)\n",
        "\n",
        "beta_initial = np.array([0, 0])\n",
        "beta = torch.zeros(2, 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "\n",
        "print(f\"Running on: {dev}\")\n",
        "\n",
        "# Define the loss function\n",
        "def ll(beta):\n",
        "    xbetahat = torch.mm(x, beta)\n",
        "    yhat = torch.sigmoid(xbetahat)\n",
        "    loss = - torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    return loss\n",
        "\n",
        "# Set up the Adam optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = Adam([beta], lr = learning_rate)\n",
        "\n",
        "# Define a function to calculate the loss and update model parameters\n",
        "def calculate_loss():\n",
        "    optimizer.zero_grad()\n",
        "    value = ll(beta)\n",
        "    value.backward()\n",
        "    return value\n",
        "\n",
        "num_iter = 100\n",
        "for i in range(1, num_iter + 1):\n",
        "  logl = optimizer.step(calculate_loss)\n",
        "  logl_value = logl.detach().item()\n",
        "  estimated_weights = beta.detach().numpy()\n",
        "  if (i + 1) % 10 == 0:\n",
        "    print(f\"Iteration {i + 1} || Log-value {logl_value}\")\n",
        "    print(f\"Estimated weights: {estimated_weights}\")\n",
        "\n",
        "estimated_weights_Adam = beta.detach().numpy()\n",
        "print(\"Estimated Parameters (Weights):\", estimated_weights_Adam)"
      ],
      "metadata": {
        "id": "sgtxsvloxubV",
        "outputId": "8e863b1e-c78b-4e29-9c00-f23429013f35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n",
            "Iteration 10 || Log-value 481.81007249688764\n",
            "Estimated weights: [[-0.06432935]\n",
            " [ 0.82784067]]\n",
            "Iteration 20 || Log-value 483.2033081110674\n",
            "Estimated weights: [[-0.17875141]\n",
            " [ 1.23860463]]\n",
            "Iteration 30 || Log-value 480.5549901483747\n",
            "Estimated weights: [[-0.11729659]\n",
            " [ 1.10106756]]\n",
            "Iteration 40 || Log-value 479.05850454399877\n",
            "Estimated weights: [[-0.11227977]\n",
            " [ 0.8888539 ]]\n",
            "Iteration 50 || Log-value 479.1051867012501\n",
            "Estimated weights: [[-0.1309616 ]\n",
            " [ 0.90463239]]\n",
            "Iteration 60 || Log-value 478.8867728450398\n",
            "Estimated weights: [[-0.13214677]\n",
            " [ 0.98764441]]\n",
            "Iteration 70 || Log-value 478.871395217095\n",
            "Estimated weights: [[-0.12641927]\n",
            " [ 0.97400056]]\n",
            "Iteration 80 || Log-value 478.8574985568922\n",
            "Estimated weights: [[-0.12406494]\n",
            " [ 0.94452985]]\n",
            "Iteration 90 || Log-value 478.8473214026619\n",
            "Estimated weights: [[-0.12461511]\n",
            " [ 0.95691198]]\n",
            "Iteration 100 || Log-value 478.8486356870633\n",
            "Estimated weights: [[-0.12538916]\n",
            " [ 0.96364576]]\n",
            "Estimated Parameters (Weights): [[-0.1252921 ]\n",
            " [ 0.96309017]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.cat((torch.tensor(train_df[[\"constant\", \"X1\"]].values, dtype=torch.float64),), dim=1)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(train_df[\"y\"].values, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.view(-1, 1)\n",
        "\n",
        "beta_initial = np.array([0, 0])\n",
        "beta = torch.zeros(2, 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "\n",
        "print(f\"Running on: {dev}\")\n",
        "\n",
        "# Define the loss function\n",
        "def ll(beta):\n",
        "    xbetahat = torch.mm(x, beta)\n",
        "    yhat = torch.sigmoid(xbetahat)\n",
        "    loss = - torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    return loss\n",
        "\n",
        "# Set up the Adam optimizer\n",
        "learning_rate = 0.12\n",
        "optimizer = LBFGS([beta], lr = learning_rate)\n",
        "\n",
        "# Define a function to calculate the loss and update model parameters\n",
        "def calculate_loss():\n",
        "    optimizer.zero_grad()\n",
        "    value = ll(beta)\n",
        "    value.backward()\n",
        "    return value\n",
        "\n",
        "num_iter = 100\n",
        "for i in range(1, num_iter + 1):\n",
        "  logl = optimizer.step(calculate_loss)\n",
        "  logl_value = logl.detach().item()\n",
        "  estimated_weights = beta.detach().numpy()\n",
        "  if (i + 1) % 10 == 0:\n",
        "    print(f\"Iteration {i + 1} || Log-value {logl_value}\")\n",
        "    print(f\"Estimated weights: {estimated_weights}\")\n",
        "\n",
        "estimated_weights_LBFGS = beta.detach().numpy()\n",
        "print(\"Estimated Parameters (Weights):\", estimated_weights_LBFGS)"
      ],
      "metadata": {
        "id": "uSqgwPQ0yWxY",
        "outputId": "d4268a54-df92-4182-fb4d-767bf3d489d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n",
            "Iteration 10 || Log-value 478.846576313701\n",
            "Estimated weights: [[-0.12596023]\n",
            " [ 0.95827163]]\n",
            "Iteration 20 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 30 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 40 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 50 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 60 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 70 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 80 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 90 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 100 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Estimated Parameters (Weights): [[-0.12596046]\n",
            " [ 0.95827329]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the initial values\n",
        "debugstart = torch.tensor([1, 2, 3], dtype = torch.float64, requires_grad = True)\n",
        "x0 = torch.tensor([1, 1/2, 1/3], dtype = torch.float64, requires_grad = True)\n",
        "y1 = torch.tensor([1], dtype = torch.float64, requires_grad = True)\n",
        "\n",
        "# Calculate xbhat\n",
        "xbhat = torch.mm(x0.unsqueeze(0), debugstart.unsqueeze(1))\n",
        "xbhat.retain_grad()\n",
        "\n",
        "# Calculate yhat\n",
        "yhat = torch.sigmoid(xbhat)\n",
        "yhat.retain_grad()\n",
        "\n",
        "# Calculate the loss\n",
        "loss = -torch.sum(y1 * torch.log(yhat) + (1 - y1) * torch.log(1 - yhat))\n",
        "loss.backward()\n",
        "\n",
        "# Print the tensors and their gradients\n",
        "print(\"y:\", y1)\n",
        "print(\"x:\", x0)\n",
        "print(\"xbhat:\", xbhat)\n",
        "print(\"Gradients of xbhat (dloss/dp):\", xbhat.grad)\n",
        "print(\"yhat:\", yhat)\n",
        "print(\"Gradients of yhat (dloss/dyhat):\", yhat.grad)\n",
        "print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "id": "5yVLjqRvyYOw",
        "outputId": "b023cb2f-1585-41fb-e9d5-e37c5340ef32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: tensor([1.], dtype=torch.float64, requires_grad=True)\n",
            "x: tensor([1.0000, 0.5000, 0.3333], dtype=torch.float64, requires_grad=True)\n",
            "xbhat: tensor([[3.]], dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "Gradients of xbhat (dloss/dp): tensor([[-0.0474]], dtype=torch.float64)\n",
            "yhat: tensor([[0.9526]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "Gradients of yhat (dloss/dyhat): tensor([[-1.0498]], dtype=torch.float64)\n",
            "Loss: 0.04858735157374191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEURAL NETWORK LOGIT"
      ],
      "metadata": {
        "id": "9XOZy2a5Ojma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "cBhQa6x3OlwS"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple logistic regression model\n",
        "class LogitModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogitModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # Single output unit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.linear.weight.dtype)  # Cast input to the same data type as the model's parameters\n",
        "        x = self.linear(x)\n",
        "        x = torch.sigmoid(x)  # Apply sigmoid activation for binary classification\n",
        "        return x\n",
        "\n",
        "x = torch.tensor(x_train, dtype = torch.float64)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(y_train, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "input_dim = x.shape[1]\n",
        "\n",
        "model = LogitModel(input_dim)\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.1)\n",
        "\n",
        "epochs = 5000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x)\n",
        "\n",
        "    outputs = outputs.float()\n",
        "    y = y.float()\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_function(outputs, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model's parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training progress\n",
        "    if epoch % (epochs / 10) == 0:\n",
        "        print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# View the learned parameters\n",
        "print(\"Learned parameters:\")\n",
        "print(f\"Model weights: {model.linear.weight[0][0]}, {model.linear.weight[0][1]}\") # these are the parameters of interest\n",
        "print(f\"Model bias:{model.linear.bias[0]}\")"
      ],
      "metadata": {
        "id": "Rs6icLu8ijPF",
        "outputId": "ef57b522-ca99-4649-8b20-e410eb05d583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/5000], Loss: 0.6244\n",
            "Epoch [500/5000], Loss: 0.5986\n",
            "Epoch [1000/5000], Loss: 0.5986\n",
            "Epoch [1500/5000], Loss: 0.5986\n",
            "Epoch [2000/5000], Loss: 0.5986\n",
            "Epoch [2500/5000], Loss: 0.5986\n",
            "Epoch [3000/5000], Loss: 0.5986\n",
            "Epoch [3500/5000], Loss: 0.5986\n",
            "Epoch [4000/5000], Loss: 0.5986\n",
            "Epoch [4500/5000], Loss: 0.5986\n",
            "Learned parameters:\n",
            "Model weights: -0.13117192685604095, 0.9582542777061462\n",
            "Model bias:0.005179565399885178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORKING PYTORCH LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "jWDJw7Ja_FuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(x_train, dtype = torch.float64)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(y_train, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Initialize parameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 200\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "# Training loop\n",
        "for i in range(num_iterations):\n",
        "    xbetahat = torch.matmul(x, beta)\n",
        "    yhat = sigmoid(xbetahat)\n",
        "\n",
        "    loss = -torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        beta -= learning_rate * beta.grad\n",
        "    beta.grad.zero_()\n",
        "\n",
        "   # if (i + 1) % 10 == 0:\n",
        "    print(f'Iteration {i+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Get the learned coefficients\n",
        "learned_beta = beta.detach().numpy()\n",
        "\n",
        "print(f'Learned Logistic Regression Coefficients (Beta): {learned_beta}')"
      ],
      "metadata": {
        "id": "fshQKecVEx2t",
        "outputId": "0284e72b-1c72-43f5-c194-10422e69b588",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Loss: 665.5518600444576\n",
            "Iteration 2, Loss: 564.8306627212372\n",
            "Iteration 3, Loss: 486.8035786649813\n",
            "Iteration 4, Loss: 479.28242033072235\n",
            "Iteration 5, Loss: 479.026635101907\n",
            "Iteration 6, Loss: 478.92136907542465\n",
            "Iteration 7, Loss: 478.8783753497714\n",
            "Iteration 8, Loss: 478.8600621132225\n",
            "Iteration 9, Loss: 478.8523052518459\n",
            "Iteration 10, Loss: 478.8490088337579\n",
            "Iteration 11, Loss: 478.84760952881254\n",
            "Iteration 12, Loss: 478.84701509608294\n",
            "Iteration 13, Loss: 478.84676267710955\n",
            "Iteration 14, Loss: 478.84665546126496\n",
            "Iteration 15, Loss: 478.8466099284955\n",
            "Iteration 16, Loss: 478.84659058933977\n",
            "Iteration 17, Loss: 478.84658237599206\n",
            "Iteration 18, Loss: 478.8465788876164\n",
            "Iteration 19, Loss: 478.84657740607713\n",
            "Iteration 20, Loss: 478.8465767768439\n",
            "Iteration 21, Loss: 478.8465765096019\n",
            "Iteration 22, Loss: 478.84657639610055\n",
            "Iteration 23, Loss: 478.84657634789517\n",
            "Iteration 24, Loss: 478.8465763274217\n",
            "Iteration 25, Loss: 478.84657631872636\n",
            "Iteration 26, Loss: 478.8465763150333\n",
            "Iteration 27, Loss: 478.8465763134649\n",
            "Iteration 28, Loss: 478.8465763127988\n",
            "Iteration 29, Loss: 478.8465763125158\n",
            "Iteration 30, Loss: 478.84657631239565\n",
            "Iteration 31, Loss: 478.84657631234467\n",
            "Iteration 32, Loss: 478.8465763123229\n",
            "Iteration 33, Loss: 478.8465763123138\n",
            "Iteration 34, Loss: 478.8465763123098\n",
            "Iteration 35, Loss: 478.8465763123082\n",
            "Iteration 36, Loss: 478.84657631230743\n",
            "Iteration 37, Loss: 478.84657631230715\n",
            "Iteration 38, Loss: 478.8465763123071\n",
            "Iteration 39, Loss: 478.846576312307\n",
            "Iteration 40, Loss: 478.846576312307\n",
            "Iteration 41, Loss: 478.846576312307\n",
            "Iteration 42, Loss: 478.8465763123069\n",
            "Iteration 43, Loss: 478.846576312307\n",
            "Iteration 44, Loss: 478.846576312307\n",
            "Iteration 45, Loss: 478.84657631230687\n",
            "Iteration 46, Loss: 478.846576312307\n",
            "Iteration 47, Loss: 478.8465763123069\n",
            "Iteration 48, Loss: 478.84657631230687\n",
            "Iteration 49, Loss: 478.84657631230687\n",
            "Iteration 50, Loss: 478.846576312307\n",
            "Iteration 51, Loss: 478.8465763123069\n",
            "Iteration 52, Loss: 478.8465763123069\n",
            "Iteration 53, Loss: 478.846576312307\n",
            "Iteration 54, Loss: 478.8465763123069\n",
            "Iteration 55, Loss: 478.84657631230687\n",
            "Iteration 56, Loss: 478.846576312307\n",
            "Iteration 57, Loss: 478.846576312307\n",
            "Iteration 58, Loss: 478.846576312307\n",
            "Iteration 59, Loss: 478.8465763123069\n",
            "Iteration 60, Loss: 478.846576312307\n",
            "Iteration 61, Loss: 478.8465763123069\n",
            "Iteration 62, Loss: 478.846576312307\n",
            "Iteration 63, Loss: 478.84657631230687\n",
            "Iteration 64, Loss: 478.846576312307\n",
            "Iteration 65, Loss: 478.8465763123069\n",
            "Iteration 66, Loss: 478.846576312307\n",
            "Iteration 67, Loss: 478.8465763123069\n",
            "Iteration 68, Loss: 478.8465763123069\n",
            "Iteration 69, Loss: 478.846576312307\n",
            "Iteration 70, Loss: 478.84657631230687\n",
            "Iteration 71, Loss: 478.846576312307\n",
            "Iteration 72, Loss: 478.846576312307\n",
            "Iteration 73, Loss: 478.846576312307\n",
            "Iteration 74, Loss: 478.8465763123069\n",
            "Iteration 75, Loss: 478.8465763123069\n",
            "Iteration 76, Loss: 478.846576312307\n",
            "Iteration 77, Loss: 478.846576312307\n",
            "Iteration 78, Loss: 478.846576312307\n",
            "Iteration 79, Loss: 478.846576312307\n",
            "Iteration 80, Loss: 478.8465763123069\n",
            "Iteration 81, Loss: 478.846576312307\n",
            "Iteration 82, Loss: 478.846576312307\n",
            "Iteration 83, Loss: 478.846576312307\n",
            "Iteration 84, Loss: 478.846576312307\n",
            "Iteration 85, Loss: 478.846576312307\n",
            "Iteration 86, Loss: 478.846576312307\n",
            "Iteration 87, Loss: 478.846576312307\n",
            "Iteration 88, Loss: 478.846576312307\n",
            "Iteration 89, Loss: 478.846576312307\n",
            "Iteration 90, Loss: 478.846576312307\n",
            "Iteration 91, Loss: 478.846576312307\n",
            "Iteration 92, Loss: 478.846576312307\n",
            "Iteration 93, Loss: 478.846576312307\n",
            "Iteration 94, Loss: 478.846576312307\n",
            "Iteration 95, Loss: 478.846576312307\n",
            "Iteration 96, Loss: 478.846576312307\n",
            "Iteration 97, Loss: 478.846576312307\n",
            "Iteration 98, Loss: 478.846576312307\n",
            "Iteration 99, Loss: 478.846576312307\n",
            "Iteration 100, Loss: 478.846576312307\n",
            "Iteration 101, Loss: 478.846576312307\n",
            "Iteration 102, Loss: 478.846576312307\n",
            "Iteration 103, Loss: 478.846576312307\n",
            "Iteration 104, Loss: 478.846576312307\n",
            "Iteration 105, Loss: 478.846576312307\n",
            "Iteration 106, Loss: 478.846576312307\n",
            "Iteration 107, Loss: 478.846576312307\n",
            "Iteration 108, Loss: 478.846576312307\n",
            "Iteration 109, Loss: 478.846576312307\n",
            "Iteration 110, Loss: 478.846576312307\n",
            "Iteration 111, Loss: 478.846576312307\n",
            "Iteration 112, Loss: 478.846576312307\n",
            "Iteration 113, Loss: 478.846576312307\n",
            "Iteration 114, Loss: 478.846576312307\n",
            "Iteration 115, Loss: 478.846576312307\n",
            "Iteration 116, Loss: 478.846576312307\n",
            "Iteration 117, Loss: 478.846576312307\n",
            "Iteration 118, Loss: 478.846576312307\n",
            "Iteration 119, Loss: 478.846576312307\n",
            "Iteration 120, Loss: 478.846576312307\n",
            "Iteration 121, Loss: 478.846576312307\n",
            "Iteration 122, Loss: 478.846576312307\n",
            "Iteration 123, Loss: 478.846576312307\n",
            "Iteration 124, Loss: 478.846576312307\n",
            "Iteration 125, Loss: 478.846576312307\n",
            "Iteration 126, Loss: 478.846576312307\n",
            "Iteration 127, Loss: 478.846576312307\n",
            "Iteration 128, Loss: 478.846576312307\n",
            "Iteration 129, Loss: 478.846576312307\n",
            "Iteration 130, Loss: 478.846576312307\n",
            "Iteration 131, Loss: 478.846576312307\n",
            "Iteration 132, Loss: 478.846576312307\n",
            "Iteration 133, Loss: 478.846576312307\n",
            "Iteration 134, Loss: 478.846576312307\n",
            "Iteration 135, Loss: 478.846576312307\n",
            "Iteration 136, Loss: 478.846576312307\n",
            "Iteration 137, Loss: 478.846576312307\n",
            "Iteration 138, Loss: 478.846576312307\n",
            "Iteration 139, Loss: 478.846576312307\n",
            "Iteration 140, Loss: 478.846576312307\n",
            "Iteration 141, Loss: 478.846576312307\n",
            "Iteration 142, Loss: 478.846576312307\n",
            "Iteration 143, Loss: 478.846576312307\n",
            "Iteration 144, Loss: 478.846576312307\n",
            "Iteration 145, Loss: 478.846576312307\n",
            "Iteration 146, Loss: 478.846576312307\n",
            "Iteration 147, Loss: 478.846576312307\n",
            "Iteration 148, Loss: 478.846576312307\n",
            "Iteration 149, Loss: 478.846576312307\n",
            "Iteration 150, Loss: 478.846576312307\n",
            "Iteration 151, Loss: 478.846576312307\n",
            "Iteration 152, Loss: 478.846576312307\n",
            "Iteration 153, Loss: 478.846576312307\n",
            "Iteration 154, Loss: 478.846576312307\n",
            "Iteration 155, Loss: 478.846576312307\n",
            "Iteration 156, Loss: 478.846576312307\n",
            "Iteration 157, Loss: 478.846576312307\n",
            "Iteration 158, Loss: 478.846576312307\n",
            "Iteration 159, Loss: 478.846576312307\n",
            "Iteration 160, Loss: 478.846576312307\n",
            "Iteration 161, Loss: 478.846576312307\n",
            "Iteration 162, Loss: 478.846576312307\n",
            "Iteration 163, Loss: 478.846576312307\n",
            "Iteration 164, Loss: 478.846576312307\n",
            "Iteration 165, Loss: 478.846576312307\n",
            "Iteration 166, Loss: 478.846576312307\n",
            "Iteration 167, Loss: 478.846576312307\n",
            "Iteration 168, Loss: 478.846576312307\n",
            "Iteration 169, Loss: 478.846576312307\n",
            "Iteration 170, Loss: 478.846576312307\n",
            "Iteration 171, Loss: 478.846576312307\n",
            "Iteration 172, Loss: 478.846576312307\n",
            "Iteration 173, Loss: 478.846576312307\n",
            "Iteration 174, Loss: 478.846576312307\n",
            "Iteration 175, Loss: 478.846576312307\n",
            "Iteration 176, Loss: 478.846576312307\n",
            "Iteration 177, Loss: 478.846576312307\n",
            "Iteration 178, Loss: 478.846576312307\n",
            "Iteration 179, Loss: 478.846576312307\n",
            "Iteration 180, Loss: 478.846576312307\n",
            "Iteration 181, Loss: 478.846576312307\n",
            "Iteration 182, Loss: 478.846576312307\n",
            "Iteration 183, Loss: 478.846576312307\n",
            "Iteration 184, Loss: 478.846576312307\n",
            "Iteration 185, Loss: 478.846576312307\n",
            "Iteration 186, Loss: 478.846576312307\n",
            "Iteration 187, Loss: 478.846576312307\n",
            "Iteration 188, Loss: 478.846576312307\n",
            "Iteration 189, Loss: 478.846576312307\n",
            "Iteration 190, Loss: 478.846576312307\n",
            "Iteration 191, Loss: 478.846576312307\n",
            "Iteration 192, Loss: 478.846576312307\n",
            "Iteration 193, Loss: 478.846576312307\n",
            "Iteration 194, Loss: 478.846576312307\n",
            "Iteration 195, Loss: 478.846576312307\n",
            "Iteration 196, Loss: 478.846576312307\n",
            "Iteration 197, Loss: 478.846576312307\n",
            "Iteration 198, Loss: 478.846576312307\n",
            "Iteration 199, Loss: 478.846576312307\n",
            "Iteration 200, Loss: 478.846576312307\n",
            "Learned Logistic Regression Coefficients (Beta): [[-0.12596081]\n",
            " [ 0.95827576]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thdBS_wYxzdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}