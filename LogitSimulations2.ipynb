{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEuqvcyxUZwKJv9UJ5pnrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stuart-lane/MachineLearning/blob/main/LogitSimulations2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CDXNFhVBDeUO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "np.random.seed(123456)\n",
        "#np.random.seed(7542716)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATING PROCESS"
      ],
      "metadata": {
        "id": "uzVdX-gmuS9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "x = np.random.randn(n)\n",
        "u = np.random.logistic(0, 1, n)\n",
        "beta0 = 0\n",
        "beta1 = 1\n",
        "\n",
        "ystar = beta0 + beta1 * x + u\n",
        "y = (ystar > 0).astype(int)\n",
        "\n",
        "#p = 1 / (1 + np.exp(-(beta0 + x*beta1 + u)))\n",
        "#y = np.random.binomial(n = 1, p = p, size = n)"
      ],
      "metadata": {
        "id": "qVd8us9nDqP_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 123)"
      ],
      "metadata": {
        "id": "4PMPLz32Ff0e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION WITH SKLEARN"
      ],
      "metadata": {
        "id": "wFPUh81huciJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "0c4MyJhDFckl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(solver = 'lbfgs', max_iter = 1000)  # You can change the solver and max_iter as needed\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "f7uIPvpvpqgs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "a5bdad2c-39c4-46df-9b57-a6ed7e54976c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intercept = model.intercept_[0]\n",
        "coefficient = model.coef_[0][1]"
      ],
      "metadata": {
        "id": "QPqhlggip0eM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Intercept: {intercept}\")\n",
        "print(f\"Coefficient for 'Regressor': {coefficient}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE2ZTPE1p4DG",
        "outputId": "61d49a48-5659-469d-a68d-bce7beb766f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: -0.12587614423279017\n",
            "Coefficient for 'Regressor': 0.9506309771391525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STANDARD LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "S0A-pPjivQEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the logistic function\n",
        "def logistic(x, beta):\n",
        "    x_beta = np.dot(x, beta)\n",
        "    x_beta = np.clip(x_beta, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x_beta))\n",
        "\n",
        "# Define the log-likelihood function\n",
        "def log_likelihood(beta, x, y):\n",
        "    p = logistic(x, beta)\n",
        "    epsilon = 1e-10\n",
        "    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "    ll = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "    return -ll\n",
        "\n",
        "def derivative(beta, x_train, y_train):\n",
        "    p = logistic(x_train, beta)\n",
        "    gradient = np.zeros(2)  # Initialize a 1D gradient array with size 2\n",
        "    gradient[0] = np.sum(y_train - p)\n",
        "    gradient[1] = np.sum(np.dot(x_train.T, y_train - p)) / x_train.shape[0]\n",
        "    return gradient\n",
        "\n",
        "beta_initial = np.array([0.5, 0.5])\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "result = minimize(log_likelihood, beta_initial, args=(x_train, y_train), method = 'L-BFGS-B', jac = derivative)\n",
        "\n",
        "print(result)\n",
        "\n",
        "estimated_beta = [f'{param:.7f}' for param in result.x]\n",
        "formatted_parameters = ', '.join(estimated_beta)\n",
        "\n",
        "print(f\"Estimated Parameters: [{formatted_parameters}]\")\n",
        "print(\"Negative Log-Likelihood:\", result.fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7XUJgrd6qJH",
        "outputId": "7758153a-79f8-4f65-e3c2-87a39d3ee47f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  message: ABNORMAL_TERMINATION_IN_LNSRCH\n",
            "  success: False\n",
            "   status: 2\n",
            "      fun: 491889.4423803657\n",
            "        x: [ 5.000e-01  5.000e-01]\n",
            "      nit: 0\n",
            "      jac: [-9.307e+04  5.957e+01]\n",
            "     nfev: 20\n",
            "     njev: 20\n",
            " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
            "Estimated Parameters: [0.5000000, 0.5000000]\n",
            "Negative Log-Likelihood: 491889.4423803657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the logistic function\n",
        "def logistic(x, beta):\n",
        "    x_beta = np.dot(x, beta)\n",
        "    x_beta = np.clip(x_beta, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x_beta))\n",
        "\n",
        "# Define the log-likelihood function\n",
        "def log_likelihood(beta, x, y):\n",
        "    p = logistic(x, beta)\n",
        "    epsilon = 1e-10\n",
        "    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "    ll = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "    return -ll\n",
        "\n",
        "# Define the gradient (Jacobian) of the log-likelihood function\n",
        "def gradient(beta, x, y):\n",
        "    p = logistic(x, beta)\n",
        "    gradient_vector = np.dot(x.T, (y - p)) / x.shape[0]\n",
        "    return -gradient_vector\n",
        "\n",
        "beta_initial = np.array([0, 1])\n",
        "\n",
        "# Example usage of minimize\n",
        "result = minimize(log_likelihood, beta_initial, args=(x_train, y_train), method='BFGS')\n",
        "\n",
        "print(result)\n",
        "\n",
        "estimated_beta = [f'{param:.7f}' for param in result.x]\n",
        "formatted_parameters = ', '.join(estimated_beta)\n",
        "\n",
        "print(f\"Estimated Parameters: [{formatted_parameters}]\")\n",
        "print(\"Negative Log-Likelihood:\", result.fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwmA-Aeg0exH",
        "outputId": "1781e3a4-efc5-420b-919b-1c5ac3dbe964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  message: Desired error not necessarily achieved due to precision loss.\n",
            "  success: False\n",
            "   status: 2\n",
            "      fun: 443516.1905556687\n",
            "        x: [-3.500e-02  1.115e-08]\n",
            "      nit: 8\n",
            "      jac: [ 0.000e+00  1.172e-02]\n",
            " hess_inv: [[ 3.128e-06  2.009e-06]\n",
            "            [ 2.009e-06  2.201e-06]]\n",
            "     nfev: 36\n",
            "     njev: 12\n",
            "Estimated Parameters: [-0.0350036, 0.0000000]\n",
            "Negative Log-Likelihood: 443516.1905556687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the logistic function\n",
        "#def logistic(x, beta):\n",
        "#    x_beta = np.dot(x, beta)\n",
        "#    x_beta = np.clip(x_beta, -500, 500)\n",
        "#    return 1 / (1 + np.exp(-x_beta))\n",
        "\n",
        "# Define the log-likelihood function\n",
        "#def log_likelihood(beta, x, y):\n",
        "#    x\n",
        "#    p = logistic(x, beta)\n",
        "#    epsilon = 1e-10\n",
        "#    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "#    ll = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "#    return -ll\n",
        "\n",
        "# Define the log-likelihood function\n",
        "def log_likelihood(beta, x, y):\n",
        "    x_beta = x @ beta\n",
        "    p = 1 / (1 + np.exp(-x_beta))\n",
        "    epsilon = 1e-10\n",
        "    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "    ll = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "    return -ll\n",
        "\n",
        "beta_initial = np.array([0, 1])\n",
        "\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "result = minimize(log_likelihood, beta_initial, args=(x_train, y_train), method = 'L-BFGS-B')\n",
        "\n",
        "print(result)\n",
        "\n",
        "estimated_beta = [f'{param:.7f}' for param in result.x]\n",
        "formatted_parameters = ', '.join(estimated_beta)\n",
        "\n",
        "print(f\"Estimated Parameters: [{formatted_parameters}]\")\n",
        "print(\"Negative Log-Likelihood:\", result.fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI-OOLF6Fbg9",
        "outputId": "dee960a4-63c9-4596-9c0e-12a78d323d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
            "  success: True\n",
            "   status: 0\n",
            "      fun: 441930.7189702924\n",
            "        x: [-1.453e-01  1.547e-05]\n",
            "      nit: 4\n",
            "      jac: [-3.783e-01  2.643e+00]\n",
            "     nfev: 15\n",
            "     njev: 5\n",
            " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
            "Estimated Parameters: [-0.1452562, 0.0000155]\n",
            "Negative Log-Likelihood: 441930.7189702924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression(X, y, learning_rate, num_iterations):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        theta -= learning_rate * gradient\n",
        "        if (i+1) % 100 == 0:\n",
        "          print(f\"Iteration: {i+1} || Gradients: {gradient[0][0]} and {gradient[1][0]}\")\n",
        "          print(f\"Estimated Parameters (Weights): {theta[0][0]} and {theta[1][0]}\")\n",
        "\n",
        "    return theta\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_iterations = 1000\n",
        "\n",
        "theta = logistic_regression(x_train, y_train, learning_rate, num_iterations)\n",
        "\n",
        "#print(\"Estimated Parameters (Weights):\", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqijV77vu_eE",
        "outputId": "1de1ed48-e8c4-4aaa-c94d-52ac114d85e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 100 || Gradients: 0.0024484652999759928 and -0.024826645218115048\n",
            "Estimated Parameters (Weights): -0.11329131281618507 and 0.81057807398994\n",
            "Iteration: 200 || Gradients: 0.0003652894496281411 and -0.00469650720890295\n",
            "Estimated Parameters (Weights): -0.12381547601229506 and 0.9287924239136948\n",
            "Iteration: 300 || Gradients: 6.741809573183612e-05 and -0.0009668692203220522\n",
            "Estimated Parameters (Weights): -0.1255479423812796 and 0.9521417298211473\n",
            "Iteration: 400 || Gradients: 1.3323826691407498e-05 and -0.00020236601955596283\n",
            "Estimated Parameters (Weights): -0.1258777871830009 and 0.9569891455894065\n",
            "Iteration: 500 || Gradients: 2.7082526901361924e-06 and -4.249818941969404e-05\n",
            "Estimated Parameters (Weights): -0.12594378392045522 and 0.9580054499044263\n",
            "Iteration: 600 || Gradients: 5.583272185494259e-07 and -8.93091069069759e-06\n",
            "Estimated Parameters (Weights): -0.1259572801211017 and 0.9582189541023278\n",
            "Iteration: 700 || Gradients: 1.1601941852526498e-07 and -1.8770430608018932e-06\n",
            "Estimated Parameters (Weights): -0.12596007182452745 and 0.9582638244779405\n",
            "Iteration: 800 || Gradients: 2.4220607813957296e-08 and -3.9451091832752583e-07\n",
            "Estimated Parameters (Weights): -0.12596065307192877 and 0.9582732551362777\n",
            "Iteration: 900 || Gradients: 5.0702631237964365e-09 and -8.291674903584956e-08\n",
            "Estimated Parameters (Weights): -0.1259607745556874 and 0.9582752372400498\n",
            "Iteration: 1000 || Gradients: 1.063120431271658e-09 and -1.7427034027586717e-08\n",
            "Estimated Parameters (Weights): -0.12596080000417947 and 0.9582756538300314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUTOGRAD"
      ],
      "metadata": {
        "id": "M_uavi5mwj0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.optim import LBFGS"
      ],
      "metadata": {
        "id": "rkjzhx7tw1dg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_keys = {'constant': x_train[:,0].flatten(), 'X1': x_train[:,1].flatten(), 'y': y_train.flatten()}\n",
        "\n",
        "train_df = pd.DataFrame(train_df_keys)"
      ],
      "metadata": {
        "id": "hABpWk9twl8k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.cat((torch.tensor(train_df[[\"constant\", \"X1\"]].values, dtype=torch.float64),), dim=1)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(train_df[\"y\"].values, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.view(-1, 1)\n",
        "\n",
        "beta_initial = np.array([0, 0])\n",
        "beta = torch.zeros(2, 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "\n",
        "print(f\"Running on: {dev}\")\n",
        "\n",
        "# Define the loss function\n",
        "def ll(beta):\n",
        "    xbetahat = torch.mm(x, beta)\n",
        "    yhat = torch.sigmoid(xbetahat)\n",
        "    loss = - torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    return loss\n",
        "\n",
        "# Set up the Adam optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = Adam([beta], lr = learning_rate)\n",
        "\n",
        "# Define a function to calculate the loss and update model parameters\n",
        "def calculate_loss():\n",
        "    optimizer.zero_grad()\n",
        "    value = ll(beta)\n",
        "    value.backward()\n",
        "    return value\n",
        "\n",
        "num_iter = 100\n",
        "for i in range(1, num_iter + 1):\n",
        "  logl = optimizer.step(calculate_loss)\n",
        "  logl_value = logl.detach().item()\n",
        "  estimated_weights = beta.detach().numpy()\n",
        "  if (i + 1) % 10 == 0:\n",
        "    print(f\"Iteration {i + 1} || Log-value {logl_value}\")\n",
        "    print(f\"Estimated weights: {estimated_weights}\")\n",
        "\n",
        "estimated_weights_Adam = beta.detach().numpy()\n",
        "print(\"Estimated Parameters (Weights):\", estimated_weights_Adam)"
      ],
      "metadata": {
        "id": "sgtxsvloxubV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "678f77d3-7d36-4fb1-f433-a16215d02b8a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n",
            "Iteration 10 || Log-value 481.81007249688764\n",
            "Estimated weights: [[-0.06432935]\n",
            " [ 0.82784067]]\n",
            "Iteration 20 || Log-value 483.2033081110674\n",
            "Estimated weights: [[-0.17875141]\n",
            " [ 1.23860463]]\n",
            "Iteration 30 || Log-value 480.5549901483747\n",
            "Estimated weights: [[-0.11729659]\n",
            " [ 1.10106756]]\n",
            "Iteration 40 || Log-value 479.05850454399877\n",
            "Estimated weights: [[-0.11227977]\n",
            " [ 0.8888539 ]]\n",
            "Iteration 50 || Log-value 479.1051867012501\n",
            "Estimated weights: [[-0.1309616 ]\n",
            " [ 0.90463239]]\n",
            "Iteration 60 || Log-value 478.8867728450398\n",
            "Estimated weights: [[-0.13214677]\n",
            " [ 0.98764441]]\n",
            "Iteration 70 || Log-value 478.871395217095\n",
            "Estimated weights: [[-0.12641927]\n",
            " [ 0.97400056]]\n",
            "Iteration 80 || Log-value 478.8574985568922\n",
            "Estimated weights: [[-0.12406494]\n",
            " [ 0.94452985]]\n",
            "Iteration 90 || Log-value 478.8473214026619\n",
            "Estimated weights: [[-0.12461511]\n",
            " [ 0.95691198]]\n",
            "Iteration 100 || Log-value 478.8486356870633\n",
            "Estimated weights: [[-0.12538916]\n",
            " [ 0.96364576]]\n",
            "Estimated Parameters (Weights): [[-0.1252921 ]\n",
            " [ 0.96309017]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.cat((torch.tensor(train_df[[\"constant\", \"X1\"]].values, dtype=torch.float64),), dim=1)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(train_df[\"y\"].values, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.view(-1, 1)\n",
        "\n",
        "beta_initial = np.array([0, 0])\n",
        "beta = torch.zeros(2, 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "\n",
        "print(f\"Running on: {dev}\")\n",
        "\n",
        "# Define the loss function\n",
        "def ll(beta):\n",
        "    xbetahat = torch.mm(x, beta)\n",
        "    yhat = torch.sigmoid(xbetahat)\n",
        "    loss = - torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    return loss\n",
        "\n",
        "# Set up the Adam optimizer\n",
        "learning_rate = 0.12\n",
        "optimizer = LBFGS([beta], lr = learning_rate)\n",
        "\n",
        "# Define a function to calculate the loss and update model parameters\n",
        "def calculate_loss():\n",
        "    optimizer.zero_grad()\n",
        "    value = ll(beta)\n",
        "    value.backward()\n",
        "    return value\n",
        "\n",
        "num_iter = 100\n",
        "for i in range(1, num_iter + 1):\n",
        "  logl = optimizer.step(calculate_loss)\n",
        "  logl_value = logl.detach().item()\n",
        "  estimated_weights = beta.detach().numpy()\n",
        "  if (i + 1) % 10 == 0:\n",
        "    print(f\"Iteration {i + 1} || Log-value {logl_value}\")\n",
        "    print(f\"Estimated weights: {estimated_weights}\")\n",
        "\n",
        "estimated_weights_LBFGS = beta.detach().numpy()\n",
        "print(\"Estimated Parameters (Weights):\", estimated_weights_LBFGS)"
      ],
      "metadata": {
        "id": "uSqgwPQ0yWxY",
        "outputId": "2e8a3d62-45e5-45ab-a82d-691fe801f03a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n",
            "Iteration 10 || Log-value 478.846576313701\n",
            "Estimated weights: [[-0.12596023]\n",
            " [ 0.95827163]]\n",
            "Iteration 20 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 30 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 40 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 50 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 60 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 70 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 80 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 90 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Iteration 100 || Log-value 478.8465763126951\n",
            "Estimated weights: [[-0.12596046]\n",
            " [ 0.95827329]]\n",
            "Estimated Parameters (Weights): [[-0.12596046]\n",
            " [ 0.95827329]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the initial values\n",
        "debugstart = torch.tensor([1, 2, 3], dtype = torch.float64, requires_grad = True)\n",
        "x0 = torch.tensor([1, 1/2, 1/3], dtype = torch.float64, requires_grad = True)\n",
        "y1 = torch.tensor([1], dtype = torch.float64, requires_grad = True)\n",
        "\n",
        "# Calculate xbhat\n",
        "xbhat = torch.mm(x0.unsqueeze(0), debugstart.unsqueeze(1))\n",
        "xbhat.retain_grad()\n",
        "\n",
        "# Calculate yhat\n",
        "yhat = torch.sigmoid(xbhat)\n",
        "yhat.retain_grad()\n",
        "\n",
        "# Calculate the loss\n",
        "loss = -torch.sum(y1 * torch.log(yhat) + (1 - y1) * torch.log(1 - yhat))\n",
        "loss.backward()\n",
        "\n",
        "# Print the tensors and their gradients\n",
        "print(\"y:\", y1)\n",
        "print(\"x:\", x0)\n",
        "print(\"xbhat:\", xbhat)\n",
        "print(\"Gradients of xbhat (dloss/dp):\", xbhat.grad)\n",
        "print(\"yhat:\", yhat)\n",
        "print(\"Gradients of yhat (dloss/dyhat):\", yhat.grad)\n",
        "print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "id": "5yVLjqRvyYOw",
        "outputId": "b023cb2f-1585-41fb-e9d5-e37c5340ef32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: tensor([1.], dtype=torch.float64, requires_grad=True)\n",
            "x: tensor([1.0000, 0.5000, 0.3333], dtype=torch.float64, requires_grad=True)\n",
            "xbhat: tensor([[3.]], dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "Gradients of xbhat (dloss/dp): tensor([[-0.0474]], dtype=torch.float64)\n",
            "yhat: tensor([[0.9526]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "Gradients of yhat (dloss/dyhat): tensor([[-1.0498]], dtype=torch.float64)\n",
            "Loss: 0.04858735157374191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEURAL NETWORK LOGIT"
      ],
      "metadata": {
        "id": "9XOZy2a5Ojma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "cBhQa6x3OlwS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple logistic regression model\n",
        "class LogitModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogitModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # Single output unit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.linear.weight.dtype)  # Cast input to the same data type as the model's parameters\n",
        "        x = self.linear(x)\n",
        "        x = torch.sigmoid(x)  # Apply sigmoid activation for binary classification\n",
        "        return x\n",
        "\n",
        "x = torch.tensor(x_train, dtype = torch.float64)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(y_train, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "input_dim = x.shape[1]\n",
        "\n",
        "model = LogitModel(input_dim)\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.1)\n",
        "\n",
        "epochs = 5000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x)\n",
        "\n",
        "    outputs = outputs.float()\n",
        "    y = y.float()\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_function(outputs, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model's parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training progress\n",
        "    if epoch % (epochs / 10) == 0:\n",
        "        print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# View the learned parameters\n",
        "print(\"Learned parameters:\")\n",
        "print(f\"Model weights: {model.linear.weight[0][0]}, {model.linear.weight[0][1]}\") # these are the parameters of interest\n",
        "print(f\"Model bias:{model.linear.bias[0]}\")"
      ],
      "metadata": {
        "id": "Rs6icLu8ijPF",
        "outputId": "cb762ddc-d24b-4970-d3b4-ee505f7c4720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/5000], Loss: 0.6476\n",
            "Epoch [500/5000], Loss: 0.5986\n",
            "Epoch [1000/5000], Loss: 0.5986\n",
            "Epoch [1500/5000], Loss: 0.5986\n",
            "Epoch [2000/5000], Loss: 0.5986\n",
            "Epoch [2500/5000], Loss: 0.5987\n",
            "Epoch [3000/5000], Loss: 0.5986\n",
            "Epoch [3500/5000], Loss: 0.5986\n",
            "Epoch [4000/5000], Loss: 0.5986\n",
            "Epoch [4500/5000], Loss: 0.5986\n",
            "Learned parameters:\n",
            "Model weights: -0.43208447098731995, 0.9582633972167969\n",
            "Model bias:0.30563104152679443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORKING PYTORCH LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "jWDJw7Ja_FuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(x_train, dtype = torch.float64)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(y_train, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Initialize parameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 10\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "# Training loop\n",
        "for i in range(num_iterations):\n",
        "    xbetahat = torch.matmul(x, beta)\n",
        "    yhat = sigmoid(xbetahat)\n",
        "\n",
        "    loss = -torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        beta -= learning_rate * beta.grad\n",
        "    beta.grad.zero_()\n",
        "\n",
        "    print(f'Iteration {i+1}, Loss: {loss.item()}')\n",
        "\n",
        "# Get the learned coefficients\n",
        "learned_beta = beta.detach().numpy()\n",
        "\n",
        "print(f'Learned Logistic Regression Coefficients (Beta): {learned_beta}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fshQKecVEx2t",
        "outputId": "b3d96847-290c-46c6-fdbd-9fe9fa6fd7d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Loss: 639.9476347509673\n",
            "Iteration 2, Loss: 480.2080403136559\n",
            "Iteration 3, Loss: 450.9563734952485\n",
            "Iteration 4, Loss: 447.88830909432954\n",
            "Iteration 5, Loss: 447.6460686936547\n",
            "Iteration 6, Loss: 447.6034053883578\n",
            "Iteration 7, Loss: 447.592129264808\n",
            "Iteration 8, Loss: 447.5890913199482\n",
            "Iteration 9, Loss: 447.58826743639116\n",
            "Iteration 10, Loss: 447.588044489895\n",
            "Learned Logistic Regression Coefficients (Beta): [[-0.06084399]\n",
            " [ 1.26671798]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05xe8X9f72k3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}