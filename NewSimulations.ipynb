{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhG0TE4UNyeg8iJebQ+JP8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stuart-lane/MachineLearning/blob/main/NewSimulations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CDXNFhVBDeUO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "\n",
        "import scipy\n",
        "from scipy.stats import norm as normal\n",
        "import sklearn\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.optim import LBFGS\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "np.random.seed(123456)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATING PROCESS"
      ],
      "metadata": {
        "id": "uzVdX-gmuS9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "\n",
        "beta0 = 0\n",
        "beta1 = 1\n",
        "\n",
        "x = np.random.randn(n)\n",
        "u = np.random.logistic(0, 1, n)\n",
        "\n",
        "ystar = beta0 + beta1 * x + u\n",
        "y = (ystar > 0).astype(int)\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "qVd8us9nDqP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOG-LIKELIHOOD FUNCTION"
      ],
      "metadata": {
        "id": "nFWmnN3N_mWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def negative_log_likelihood(beta, X, y):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        xbhat = np.dot(X, beta)\n",
        "        yhat = 1 / (1 + np.exp(-xbhat))\n",
        "        loss = -np.sum(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))\n",
        "    elif isinstance(X, torch.Tensor):\n",
        "        xbhat = torch.mm(X, beta)\n",
        "        yhat = torch.sigmoid(xbhat)\n",
        "        loss = -torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Use either numpy array or torch tensor.\")\n",
        "    return loss"
      ],
      "metadata": {
        "id": "o2NioJ4g_lvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATING PROCESS"
      ],
      "metadata": {
        "id": "mEpePXoWHbd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "\n",
        "beta0 = 0\n",
        "beta1 = 1\n",
        "sigma = 1\n",
        "beta = np.array([beta0, beta1])\n",
        "\n",
        "x = np.random.randn(n)\n",
        "u = np.random.normal(0, sigma, n)\n",
        "\n",
        "y_cutoff = 0\n",
        "\n",
        "ystar = beta0 + beta1 * x + u\n",
        "yind = (ystar > y_cutoff).astype(int)\n",
        "y = yind * ystar\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "MCU0k3iZHbd0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOG-LIKELIHOOD FUNCTION"
      ],
      "metadata": {
        "id": "22mDTWeKHbd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_pdf(X, mu, sigma = 1):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        constant = 1.0 / np.sqrt(2 * math.pi * sigma**2)\n",
        "        exponent = -((X - mu)**2) / (2 * sigma**2)\n",
        "        return constant * math.exp(exponent)\n",
        "    elif isinstance(X, torch.Tensor):\n",
        "        constant = 1.0 / torch.sqrt(2 * torch.tensor(math.pi) * sigma**2)\n",
        "        exponent = -((X - mu)**2) / (2 * sigma**2)\n",
        "        return constant * torch.exp(exponent)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Use either numpy array or torch tensor.\")\n",
        "    return loss\n",
        "\n",
        "\n",
        "def negative_log_likelihood(beta, X, y, yind, y_cutoff, sigma = 1):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        xbeta = np.dot(X, beta)\n",
        "        term1 = (y - xbeta) / sigma\n",
        "        normpdf = normal_pdf(term1, 0, 1) / sigma\n",
        "        cdf_term2 = normal.cdf((xbeta - y_cutoff) / sigma)\n",
        "        loss = - (np.sum(yind * np.log(normpdf) + (1 - yind) * np.log(1 - (cdf_term2))))\n",
        "    elif isinstance(X, torch.Tensor):\n",
        "        xbeta = torch.matmul(X, beta)\n",
        "        term1 = (y - xbeta) / sigma\n",
        "        normpdf = normal_pdf(term1, 0, 1) / sigma\n",
        "        cdf_term2 = (xbeta - y_cutoff) / sigma\n",
        "        loss = - (torch.sum(yind * torch.log(normpdf) + (1 - yind) * torch.log(1 - cdf_term2)))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Use either numpy array or torch tensor.\")\n",
        "    return loss"
      ],
      "metadata": {
        "id": "BmuQRAFOHbd0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA FORMATTING"
      ],
      "metadata": {
        "id": "fXePQkzxB5bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 123)\n",
        "\n",
        "train_df_keys = {'constant': x_train[:,0].flatten(), 'X1': x_train[:,1].flatten(), 'y': y_train.flatten()}\n",
        "train_df = pd.DataFrame(train_df_keys)"
      ],
      "metadata": {
        "id": "4PMPLz32Ff0e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the variables to torch tensors\n",
        "x = torch.tensor(x_train, dtype = torch.float64, requires_grad = True)\n",
        "y = torch.tensor(y_train, dtype = torch.float64, requires_grad = True)\n",
        "yi = torch.tensor(yind, dtype = torch.float64, requires_grad = True)\n",
        "yc = torch.tensor(y_cutoff, dtype = torch.float64, requires_grad = True)\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float64, requires_grad = True)\n",
        "\n",
        "print(beta)"
      ],
      "metadata": {
        "id": "PYz479c1AeKX",
        "outputId": "e81197f4-844d-4355-ca6c-9811facf8428",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2765],\n",
            "        [1.8134]], dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION WITH SKLEARN"
      ],
      "metadata": {
        "id": "wFPUh81huciJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(solver = 'lbfgs', max_iter = 1000)  # You can change the solver and max_iter as needed\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "intercept = model.intercept_[0]\n",
        "coefficient = model.coef_[0][1]"
      ],
      "metadata": {
        "id": "f7uIPvpvpqgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Intercept: {intercept}\")\n",
        "print(f\"Coefficient for 'Regressor': {coefficient}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE2ZTPE1p4DG",
        "outputId": "5d8259e1-b1f4-43be-c90a-db1392623c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: -0.12587614423279017\n",
            "Coefficient for 'Regressor': 0.9506309771391525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STANDARD LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "S0A-pPjivQEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression(X, y, learning_rate, num_iterations):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        theta -= learning_rate * gradient\n",
        "        if (i+1) % 250 == 0:\n",
        "          print(f\"Iteration: {i+1} || Gradients: {gradient[0][0]} and {gradient[1][0]}\")\n",
        "          print(f\"Estimated Parameters (Weights): {theta[0][0]} and {theta[1][0]}\")\n",
        "\n",
        "    return theta\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_iterations = 1000\n",
        "\n",
        "theta = logistic_regression(x_train, y_train, learning_rate, num_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqijV77vu_eE",
        "outputId": "20db4cdb-6d72-4100-ce49-060a35117a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 250 || Gradients: -0.00017192177053207462 and -0.002321160390690911\n",
            "Estimated Parameters (Weights): 0.061198562140810986 and 0.9318501526068228\n",
            "Iteration: 500 || Gradients: -3.739478394055376e-06 and -5.142163406040949e-05\n",
            "Estimated Parameters (Weights): 0.062283941471408026 and 0.9465825388605845\n",
            "Iteration: 750 || Gradients: -8.340395103632881e-08 and -1.1531930849111027e-06\n",
            "Estimated Parameters (Weights): 0.06230777791217243 and 0.9469109376872196\n",
            "Iteration: 1000 || Gradients: -1.868580222913474e-09 and -2.5868296530534572e-08\n",
            "Estimated Parameters (Weights): 0.06230831039952592 and 0.9469183033956795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUTOGRAD"
      ],
      "metadata": {
        "id": "M_uavi5mwj0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "\n",
        "print(f\"Running on: {dev}\")\n",
        "\n",
        "# Set up the Adam optimizer\n",
        "learning_rate = 0.01\n",
        "optimizer = LBFGS([beta], lr = learning_rate)\n",
        "\n",
        "#negative_log_likelihood(beta, x, y, yi, yc, sigma = 1)\n",
        "\n",
        "# Define a function to calculate the loss and update model parameters\n",
        "def calculate_loss():\n",
        "    optimizer.zero_grad()\n",
        "    value = negative_log_likelihood(beta, x, y, yi, yc)\n",
        "    value.backward()\n",
        "    return value\n",
        "\n",
        "num_iter = 10\n",
        "for i in range(1, num_iter + 1):\n",
        "  logl = optimizer.step(calculate_loss)\n",
        "  logl_value = logl.detach().item()\n",
        "  estimated_weights = beta.detach().numpy()\n",
        "  if (i + 1) % 25 == 0:\n",
        "    print(f\"Iteration {i + 1} || Log-value {logl_value}\")\n",
        "    print(f\"Estimated weights: {estimated_weights}\")\n",
        "\n",
        "estimated_weights_LBFGS = beta.detach().numpy()\n",
        "print(\"Estimated Parameters (Weights):\", estimated_weights_LBFGS)"
      ],
      "metadata": {
        "id": "uSqgwPQ0yWxY",
        "outputId": "f2c54d6e-d74d-4c60-e7b4-5149e3f7974e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n",
            "Iteration 2 || Log-value nan\n",
            "Estimated weights: [[0.3250124]\n",
            " [1.7104483]]\n",
            "Iteration 3 || Log-value nan\n",
            "Estimated weights: [[0.36337844]\n",
            " [1.61112273]]\n",
            "Iteration 4 || Log-value nan\n",
            "Estimated weights: [[0.39406622]\n",
            " [1.53201509]]\n",
            "Iteration 5 || Log-value nan\n",
            "Estimated weights: [[0.41865421]\n",
            " [1.46884336]]\n",
            "Iteration 6 || Log-value nan\n",
            "Estimated weights: [[0.43839354]\n",
            " [1.41825729]]\n",
            "Iteration 7 || Log-value nan\n",
            "Estimated weights: [[0.45474395]\n",
            " [1.37590146]]\n",
            "Iteration 8 || Log-value nan\n",
            "Estimated weights: [[0.46746403]\n",
            " [1.34339297]]\n",
            "Iteration 9 || Log-value nan\n",
            "Estimated weights: [[0.47774081]\n",
            " [1.31724737]]\n",
            "Iteration 10 || Log-value nan\n",
            "Estimated weights: [[0.48605541]\n",
            " [1.29613295]]\n",
            "Iteration 11 || Log-value nan\n",
            "Estimated weights: [[0.49279343]\n",
            " [1.27904007]]\n",
            "Estimated Parameters (Weights): [[0.49279343]\n",
            " [1.27904007]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEURAL NETWORK LOGIT"
      ],
      "metadata": {
        "id": "9XOZy2a5Ojma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogitModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogitModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # Single output unit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.linear.weight.dtype)  # Cast input to the same data type as the model's parameters\n",
        "        x = self.linear(x)\n",
        "        x = torch.sigmoid(x)  # Apply sigmoid activation for binary classification\n",
        "        return x\n",
        "\n",
        "input_dim = x.shape[1]\n",
        "\n",
        "model = LogitModel(input_dim)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "epochs = 5000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs+1):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "    outputs = model(x)\n",
        "    outputs = outputs.float()\n",
        "    y = y.float()\n",
        "\n",
        "    loss = loss_function(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % (epochs / 10) == 0:\n",
        "        print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# View the learned parameters\n",
        "print(\"Learned parameters:\")\n",
        "print(f\"Model weights: {model.linear.weight[0][0]}, {model.linear.weight[0][1]}\") # these are the parameters of interest\n",
        "print(f\"Model bias:{model.linear.bias[0]}\")"
      ],
      "metadata": {
        "id": "Rs6icLu8ijPF",
        "outputId": "5e3f3fc0-e5e6-4dca-a6b2-9f09092d6b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/5000], Loss: 0.6721\n",
            "Epoch [500/5000], Loss: 0.6202\n",
            "Epoch [1000/5000], Loss: 0.6087\n",
            "Epoch [1500/5000], Loss: 0.6069\n",
            "Epoch [2000/5000], Loss: 0.6067\n",
            "Epoch [2500/5000], Loss: 0.6067\n",
            "Epoch [3000/5000], Loss: 0.6067\n",
            "Epoch [3500/5000], Loss: 0.6067\n",
            "Epoch [4000/5000], Loss: 0.6067\n",
            "Epoch [4500/5000], Loss: 0.6067\n",
            "Epoch [5000/5000], Loss: 0.6067\n",
            "Learned parameters:\n",
            "Model weights: -0.3844618499279022, 0.9469168186187744\n",
            "Model bias:0.4467701315879822\n"
          ]
        }
      ]
    }
  ]
}