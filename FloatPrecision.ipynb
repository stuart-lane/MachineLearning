{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+LXIorMOtAzrINeO9flAF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stuart-lane/MachineLearning/blob/main/FloatPrecision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tG4RH0NrBFIM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "\n",
        "import scipy\n",
        "from scipy.stats import norm as normal\n",
        "import sklearn\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.optim import LBFGS\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "np.random.seed(123456)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATING PROCESS"
      ],
      "metadata": {
        "id": "qrWlsO_8BNkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "\n",
        "beta0 = 0\n",
        "beta1 = 1\n",
        "\n",
        "x = np.random.randn(n)\n",
        "u = np.random.logistic(0, 1, n)\n",
        "\n",
        "ystar = beta0 + beta1 * x + u\n",
        "y = (ystar > 0).astype(int)\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "qVd8us9nDqP_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOG-LIKELIHOOD FUNCTION"
      ],
      "metadata": {
        "id": "nFWmnN3N_mWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def negative_log_likelihood(beta, X, y):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        xbhat = np.dot(X, beta)\n",
        "        yhat = 1 / (1 + np.exp(-xbhat))\n",
        "        loss = -np.sum(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))\n",
        "    elif isinstance(X, torch.Tensor):\n",
        "        xbhat = torch.mm(X, beta)\n",
        "        yhat = torch.sigmoid(xbhat)\n",
        "        loss = -torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Use either numpy array or torch tensor.\")\n",
        "    return loss"
      ],
      "metadata": {
        "id": "o2NioJ4g_lvn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ekN3YVcGBQi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "\n",
        "beta0 = 0\n",
        "beta1 = 1\n",
        "\n",
        "x = np.random.randn(n)\n",
        "u = np.random.logistic(0, 1, n)\n",
        "\n",
        "ystar = beta0 + beta1 * x + u\n",
        "y = (ystar > 0).astype(int)\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "B9ixh9fPBTZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOG-LIKELIHOOD FUNCTION"
      ],
      "metadata": {
        "id": "EpKR_6GdBTZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def negative_log_likelihood(beta, X, y):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        xbhat = np.dot(X, beta)\n",
        "        yhat = 1 / (1 + np.exp(-xbhat))\n",
        "        loss = -np.sum(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))\n",
        "    elif isinstance(X, torch.Tensor):\n",
        "        xbhat = torch.mm(X, beta)\n",
        "        yhat = torch.sigmoid(xbhat)\n",
        "        loss = -torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Use either numpy array or torch tensor.\")\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Ryn1dfZ2BTZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATING PROCESS"
      ],
      "metadata": {
        "id": "mEpePXoWHbd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "\n",
        "beta0 = 0.5\n",
        "beta1 = 3\n",
        "sigma = 1\n",
        "beta = np.array([beta0, beta1])\n",
        "\n",
        "x = np.random.randn(n)\n",
        "u = np.random.logistic(0, 1, n)\n",
        "\n",
        "y = beta0 + beta1 * x + u\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "Nt2Hc3hNlwlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def negative_log_likelihood(beta, X, y):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        xbeta = np.dot(X, beta)\n",
        "        expxbeta = np.exp(-y + xbeta)\n",
        "        loss = -np.sum((-(y - xbeta) -  2 * np.log(1 + expxbeta)))\n",
        "    elif isinstance(X, torch.Tensor):\n",
        "        xbeta = torch.mm(X, beta)\n",
        "        expxbeta = torch.exp(-y + xbeta)\n",
        "        loss = -torch.sum(-(y - xbeta) -  2 * torch.log(1 + expxbeta))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Use either numpy array or torch tensor.\")\n",
        "    return loss"
      ],
      "metadata": {
        "id": "iQc9i2Pjl4rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATING PROCESS"
      ],
      "metadata": {
        "id": "yUq23iKblw31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "\n",
        "beta0 = 0\n",
        "beta1 = 1\n",
        "sigma = 1\n",
        "beta = np.array([beta0, beta1])\n",
        "\n",
        "x = np.random.randn(n)\n",
        "u = np.random.normal(0, sigma, n)\n",
        "\n",
        "y_cutoff = 0\n",
        "\n",
        "ystar = beta0 + beta1 * x + u\n",
        "yind = (ystar > y_cutoff).astype(int)\n",
        "y = yind * ystar\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "MCU0k3iZHbd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOG-LIKELIHOOD FUNCTION"
      ],
      "metadata": {
        "id": "22mDTWeKHbd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normal_pdf(X, mu, sigma = 1):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        constant = 1.0 / np.sqrt(2 * math.pi * sigma**2)\n",
        "        exponent = -((X - mu)**2) / (2 * sigma**2)\n",
        "        return constant * math.exp(exponent)\n",
        "    elif isinstance(X, torch.Tensor):\n",
        "        constant = 1.0 / torch.sqrt(2 * torch.tensor(math.pi) * sigma**2)\n",
        "        exponent = -((X - mu)**2) / (2 * sigma**2)\n",
        "        return constant * torch.exp(exponent)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Use either numpy array or torch tensor.\")\n",
        "    return loss\n",
        "\n",
        "\n",
        "def negative_log_likelihood(beta, X, y, yind, y_cutoff, sigma = 1):\n",
        "    if isinstance(X, np.ndarray):\n",
        "        xbeta = np.dot(X, beta)\n",
        "        term1 = (y - xbeta) / sigma\n",
        "        normpdf = normal_pdf(term1, 0, 1) / sigma\n",
        "        cdf_term2 = normal.cdf((xbeta - y_cutoff) / sigma)\n",
        "        loss = - (np.sum(yind * np.log(normpdf) + (1 - yind) * np.log(1 - (cdf_term2))))\n",
        "    elif isinstance(X, torch.Tensor):\n",
        "        xbeta = torch.matmul(X, beta)\n",
        "        term1 = (y - xbeta) / sigma\n",
        "        normpdf = normal_pdf(term1, 0, 1) / sigma\n",
        "        cdf_term2 = (xbeta - y_cutoff) / sigma\n",
        "        loss = - (torch.sum(yind * torch.log(normpdf) + (1 - yind) * torch.log(1 - cdf_term2)))\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported input type. Use either numpy array or torch tensor.\")\n",
        "    return loss"
      ],
      "metadata": {
        "id": "BmuQRAFOHbd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA FORMATTING"
      ],
      "metadata": {
        "id": "fXePQkzxB5bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 123)\n",
        "\n",
        "train_df_keys = {'constant': x_train[:,0].flatten(), 'X1': x_train[:,1].flatten(), 'y': y_train.flatten()}\n",
        "train_df = pd.DataFrame(train_df_keys)"
      ],
      "metadata": {
        "id": "4PMPLz32Ff0e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the variables to torch tensors\n",
        "x = torch.tensor(x_train, dtype = torch.float64, requires_grad = True)\n",
        "y = torch.tensor(y_train, dtype = torch.float64, requires_grad = True)\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float64, requires_grad = True)\n",
        "\n",
        "print(beta)"
      ],
      "metadata": {
        "id": "PYz479c1AeKX",
        "outputId": "b56f659b-7cd0-4a4a-d428-9ce09700235a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4395],\n",
            "        [ 0.9812]], dtype=torch.float64, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "\n",
        "print(f\"Running on: {dev}\")\n",
        "\n",
        "# Set up the Adam optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = LBFGS([beta], lr = learning_rate)\n",
        "\n",
        "# Define a function to calculate the loss and update model parameters\n",
        "def calculate_loss():\n",
        "    optimizer.zero_grad()\n",
        "    value = negative_log_likelihood(beta, x, y)\n",
        "    value.backward()\n",
        "    return value\n",
        "\n",
        "num_iter = 100\n",
        "for i in range(1, num_iter + 1):\n",
        "  logl = optimizer.step(calculate_loss)\n",
        "  logl_value = logl.detach().item()\n",
        "  estimated_weights = beta.detach().numpy()\n",
        "  print(f\"Iteration {i + 1} || Log-value {logl_value}\")\n",
        "  print(f\"Estimated weights: {estimated_weights}\")\n",
        "\n",
        "estimated_weights_LBFGS = beta.detach().numpy()\n",
        "print(\"Estimated Parameters (Weights):\", estimated_weights_LBFGS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYDmd1h7BUrd",
        "outputId": "b6df4cb8-1936-414d-9633-c86706f6ce84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n",
            "Iteration 2 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.12595029]\n",
            " [ 0.9582468 ]]\n",
            "Iteration 3 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.12595384]\n",
            " [ 0.9582568 ]]\n",
            "Iteration 4 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.1259552 ]\n",
            " [ 0.95826036]]\n",
            "Iteration 5 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.1259567]\n",
            " [ 0.9582646]]\n",
            "Iteration 6 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.12595715]\n",
            " [ 0.9582657 ]]\n",
            "Iteration 7 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.12595783]\n",
            " [ 0.95826757]]\n",
            "Iteration 8 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.1259584]\n",
            " [ 0.9582692]]\n",
            "Iteration 9 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.12595864]\n",
            " [ 0.9582699 ]]\n",
            "Iteration 10 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.12595885]\n",
            " [ 0.9582705 ]]\n",
            "Iteration 11 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.12595904]\n",
            " [ 0.958271  ]]\n",
            "Iteration 12 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.12595922]\n",
            " [ 0.9582715 ]]\n",
            "Iteration 13 || Log-value 478.8465576171875\n",
            "Estimated weights: [[-0.12595952]\n",
            " [ 0.95827234]]\n",
            "Iteration 14 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595963]\n",
            " [ 0.9582727 ]]\n",
            "Iteration 15 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595975]\n",
            " [ 0.958273  ]]\n",
            "Iteration 16 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 17 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 18 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 19 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 20 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 21 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 22 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 23 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 24 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 25 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 26 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 27 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 28 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 29 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 30 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 31 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 32 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 33 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 34 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 35 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 36 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 37 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 38 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 39 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 40 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 41 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 42 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 43 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 44 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 45 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 46 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 47 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 48 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 49 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 50 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 51 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 52 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 53 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 54 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 55 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 56 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 57 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 58 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 59 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 60 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 61 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 62 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 63 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 64 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 65 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 66 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 67 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 68 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 69 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 70 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 71 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 72 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 73 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 74 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 75 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 76 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 77 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 78 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 79 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 80 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 81 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 82 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 83 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 84 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 85 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 86 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 87 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 88 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 89 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 90 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 91 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 92 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 93 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 94 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 95 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 96 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 97 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 98 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 99 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 100 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Iteration 101 || Log-value 478.8465881347656\n",
            "Estimated weights: [[-0.12595986]\n",
            " [ 0.9582733 ]]\n",
            "Estimated Parameters (Weights): [[-0.12595986]\n",
            " [ 0.9582733 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the variables to torch tensors\n",
        "x = torch.tensor(x_train, dtype = torch.float16, requires_grad = True)\n",
        "y = torch.tensor(y_train, dtype = torch.float16, requires_grad = True)\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float16, requires_grad = True)"
      ],
      "metadata": {
        "id": "K2DoPwsyBZuf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "\n",
        "print(f\"Running on: {dev}\")\n",
        "\n",
        "# Set up the Adam optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = LBFGS([beta], lr = learning_rate)\n",
        "\n",
        "# Define a function to calculate the loss and update model parameters\n",
        "def calculate_loss():\n",
        "    optimizer.zero_grad()\n",
        "    value = negative_log_likelihood(beta, x, y)\n",
        "    value.backward()\n",
        "    return value\n",
        "\n",
        "num_iter = 100\n",
        "for i in range(1, num_iter + 1):\n",
        "  logl = optimizer.step(calculate_loss)\n",
        "  logl_value = logl.detach().item()\n",
        "  estimated_weights = beta.detach().numpy()\n",
        "  print(f\"Iteration {i + 1} || Log-value {logl_value}\")\n",
        "  print(f\"Estimated weights: {estimated_weights}\")\n",
        "\n",
        "estimated_weights_LBFGS = beta.detach().numpy()\n",
        "print(\"Estimated Parameters (Weights):\", estimated_weights_LBFGS)"
      ],
      "metadata": {
        "id": "ed7M00WGBfts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Function definition\n",
        "def f(x):\n",
        "    return x**3 - 3 * x**2 + x\n",
        "\n",
        "# Values\n",
        "x_float32 = torch.tensor(1.0, dtype=torch.float32)\n",
        "x_float64 = torch.tensor(1.0, dtype=torch.float64)\n",
        "\n",
        "# Derivatives using centered finite difference\n",
        "delta = 1e-5\n",
        "f_prime_float32 = (f(x_float32 + delta) - f(x_float32 - delta)) / (2 * delta)\n",
        "f_prime_float64 = (f(x_float64 + delta) - f(x_float64 - delta)) / (2 * delta)\n",
        "\n",
        "print(\"Derivative at x=1 using float32 precision:\", f_prime_float32)\n",
        "print(\"Derivative at x=1 using float64 precision:\", f_prime_float64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fc1mbsjBhPG",
        "outputId": "ec32a175-984c-41a1-e98e-afb965778c3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Derivative at x=1 using float32 precision: tensor(-2.0027)\n",
            "Derivative at x=1 using float64 precision: tensor(-2.0000, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Simulated data\n",
        "mu = 10  # known mean\n",
        "sigma_true = 2  # true standard deviation\n",
        "data_start = torch.normal(mu, sigma_true, size=(10000,))\n",
        "\n",
        "data32 = data_start\n",
        "data64 = data_start.to(torch.float64)"
      ],
      "metadata": {
        "id": "01uO_n_xC9Y2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Negative Log Likelihood function\n",
        "def neg_log_likelihood(sigma, data, mu):\n",
        "    return -torch.sum(torch.log(1 / (torch.sqrt(2 * torch.tensor(3.1415)) * sigma)) - ((data - mu) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "# Parameter to optimize\n",
        "sigma = torch.tensor(1.0, requires_grad=True, dtype=torch.float64)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD([sigma], lr=1)\n",
        "\n",
        "\n",
        "# Maximum likelihood estimation\n",
        "num_iterations = 1000\n",
        "for i in range(num_iterations):\n",
        "    optimizer.zero_grad()\n",
        "    loss = neg_log_likelihood(sigma, data32, mu)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Maximum Likelihood Estimator (sigma):\", sigma.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o83H9-5BDG6h",
        "outputId": "f8dcbf30-1fe6-42f2-a22f-361b4e137526"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Likelihood Estimator (sigma): 29444.701222334916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Maximum Likelihood Estimator (sigma):\", sigma.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiPpDLODEsLx",
        "outputId": "8971c9bf-b586-416c-f881-02397455efa0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Likelihood Estimator (sigma): 96554.55653534055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sigma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptwL8LtkEZI5",
        "outputId": "bbf6ef52-1def-47a6-fa06-0a463a4a59ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(26383.0632, dtype=torch.float64, requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Simulated data\n",
        "mu = 10  # known mean\n",
        "sigma_true = 3.5  # true standard deviation\n",
        "data = torch.normal(mu, sigma_true, size=(1000,))\n",
        "\n",
        "# Negative Log Likelihood function\n",
        "def neg_log_likelihood(sigma, data, mu):\n",
        "    return -torch.sum(torch.log(1 / (torch.sqrt(2 * torch.tensor(3.1415)) * sigma)) + ((data - mu) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "# Parameter to optimize\n",
        "sigma = torch.tensor(1.0, requires_grad=True, dtype=torch.float64)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD([sigma], lr=0.0000001)\n",
        "\n",
        "# Maximum likelihood estimation\n",
        "num_iterations = 1000\n",
        "for i in range(num_iterations):\n",
        "    optimizer.zero_grad()\n",
        "    loss = neg_log_likelihood(sigma, data, mu)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Maximum Likelihood Estimator (sigma):\", sigma.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKABWDJ8EwXy",
        "outputId": "13f2197c-4cdd-4795-e566-45ce52b8260a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Likelihood Estimator (sigma): -2.3334010925459867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5CoCHTgXG_Sn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}