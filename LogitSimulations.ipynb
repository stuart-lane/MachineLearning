{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmMOD+s415R1AHJ+Axx1TZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stuart-lane/MachineLearning/blob/main/LogitSimulations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CDXNFhVBDeUO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "np.random.seed(123456)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATING PROCESS"
      ],
      "metadata": {
        "id": "uzVdX-gmuS9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "\n",
        "x = np.random.randn(n)\n",
        "u = np.random.randn(n)\n",
        "beta0 = 0\n",
        "beta1 = 1\n",
        "\n",
        "p = 1 / (1 + np.exp(-(beta0 + x*beta1 + u)))\n",
        "y = np.random.binomial(n = 1, p = p, size = n)"
      ],
      "metadata": {
        "id": "qVd8us9nDqP_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "u = u.reshape(-1, 1)\n",
        "x = np.hstack((np.ones((n, 1)), x.reshape(-1, 1)))\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5, random_state = 123)"
      ],
      "metadata": {
        "id": "4PMPLz32Ff0e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION WITH SKLEARN"
      ],
      "metadata": {
        "id": "wFPUh81huciJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "0c4MyJhDFckl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(solver='lbfgs', max_iter=1000)  # You can change the solver and max_iter as needed\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "f7uIPvpvpqgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intercept = model.intercept_[0]\n",
        "coefficient = model.coef_[0][1]"
      ],
      "metadata": {
        "id": "QPqhlggip0eM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Intercept: {intercept}\")\n",
        "print(f\"Coefficient for 'Regressor': {coefficient}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE2ZTPE1p4DG",
        "outputId": "4a50de76-240f-4096-9f1e-4afb737514b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 0.13604864738861402\n",
            "Coefficient for 'Regressor': 1.0227435494366917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STANDARD LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "S0A-pPjivQEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the logistic function\n",
        "def logistic(x, beta):\n",
        "    x_beta = np.dot(x, beta)\n",
        "    x_beta = np.clip(x_beta, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x_beta))\n",
        "\n",
        "# Define the log-likelihood function\n",
        "def log_likelihood(beta, x, y):\n",
        "    p = logistic(x, beta)\n",
        "    epsilon = 1e-10\n",
        "    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "    ll = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
        "    return -ll\n",
        "\n",
        "beta_initial = np.array([0, 0])\n",
        "\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "result = minimize(log_likelihood, beta_initial, args=(x_train, y_train), method = 'CG')\n",
        "\n",
        "print(result)\n",
        "\n",
        "estimated_beta = [f'{param:.7f}' for param in result.x]\n",
        "formatted_parameters = ', '.join(estimated_beta)\n",
        "\n",
        "print(f\"Estimated Parameters: [{formatted_parameters}]\")\n",
        "print(\"Negative Log-Likelihood:\", result.fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI-OOLF6Fbg9",
        "outputId": "f9cacc43-c948-4695-bc69-6dad5826a31d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " message: Optimization terminated successfully.\n",
            " success: True\n",
            "  status: 0\n",
            "     fun: 173158.77328570024\n",
            "       x: [ 6.402e-02 -2.021e-08]\n",
            "     nit: 5\n",
            "     jac: [ 0.000e+00  0.000e+00]\n",
            "    nfev: 36\n",
            "    njev: 12\n",
            "Estimated Parameters: [0.0640219, -0.0000000]\n",
            "Negative Log-Likelihood: 173158.77328570024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression(X, y, learning_rate, num_iterations):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros((n, 1))\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        gradient = np.dot(X.T, (h - y)) / m\n",
        "        theta -= learning_rate * gradient\n",
        "\n",
        "    return theta\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_iterations = 1000\n",
        "\n",
        "theta = logistic_regression(x_train, y_train, learning_rate, num_iterations)\n",
        "\n",
        "print(\"Estimated Parameters (Weights):\", theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqijV77vu_eE",
        "outputId": "3a17f850-0111-43d9-dea5-c1829ced6655"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Parameters (Weights): [[0.13685842]\n",
            " [1.03705832]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUTOGRAD"
      ],
      "metadata": {
        "id": "M_uavi5mwj0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import LBFGS\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "rkjzhx7tw1dg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_keys = {'constant': x_train[:,0].flatten(), 'X1': x_train[:,1].flatten(), 'y': y_train.flatten()}\n",
        "\n",
        "train_df = pd.DataFrame(train_df_keys)"
      ],
      "metadata": {
        "id": "hABpWk9twl8k"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.cat((torch.tensor(train_df[[\"constant\", \"X1\"]].values, dtype=torch.float64),), dim=1)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(train_df[\"y\"].values, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "\n",
        "beta_initial = np.array([0, 0])\n",
        "beta = torch.zeros(2, 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    dev = \"cuda\"\n",
        "else:\n",
        "    dev = \"cpu\"\n",
        "\n",
        "print(f\"Running on: {dev}\")"
      ],
      "metadata": {
        "id": "K5OhqqElxQHW",
        "outputId": "bc3ababb-ba3e-45a1-b7ca-3982c123da28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "def ll(beta):\n",
        "    xbetahat = torch.mm(x, beta)\n",
        "    yhat = torch.sigmoid(xbetahat)\n",
        "    loss = - torch.sum(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat))\n",
        "    return loss\n",
        "\n",
        "# Set up the Adam optimizer\n",
        "learning_rate = 0.01\n",
        "optimizer = LBFGS([beta], lr = learning_rate)\n",
        "\n",
        "# Define a function to calculate the loss and update model parameters\n",
        "def calculate_loss():\n",
        "    optimizer.zero_grad()\n",
        "    value = ll(beta)\n",
        "    value.backward()\n",
        "    return value"
      ],
      "metadata": {
        "id": "sgtxsvloxubV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 10):\n",
        "  logl = optimizer.step(calculate_loss)\n",
        "  logl_value = logl.detach().item()\n",
        "  estimated_weights = beta.detach().numpy()\n",
        "  print(f\"{i}: {logl_value}\")\n",
        "  print(f\"{i}: {estimated_weights}\")\n",
        "\n",
        "estimated_weights = beta.detach().numpy()\n",
        "print(\"Estimated Parameters (Weights):\", estimated_weights)"
      ],
      "metadata": {
        "id": "uSqgwPQ0yWxY",
        "outputId": "ec31c2b7-03f5-4d79-c61f-dc2a84f062bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 372.34156723584204\n",
            "1: [[-0.74413014]\n",
            " [ 0.14729873]]\n",
            "2: 372.34156723584204\n",
            "2: [[-0.74413014]\n",
            " [ 0.14729873]]\n",
            "3: 372.34156723584204\n",
            "3: [[-0.74413014]\n",
            " [ 0.14729873]]\n",
            "4: 372.34156723584204\n",
            "4: [[-0.74413014]\n",
            " [ 0.14729873]]\n",
            "5: 372.34156723584204\n",
            "5: [[-0.74413014]\n",
            " [ 0.14729873]]\n",
            "6: 372.34156723584204\n",
            "6: [[-0.74413014]\n",
            " [ 0.14729873]]\n",
            "7: 372.34156723584204\n",
            "7: [[-0.74413014]\n",
            " [ 0.14729873]]\n",
            "8: 372.34156723584204\n",
            "8: [[-0.74413014]\n",
            " [ 0.14729873]]\n",
            "9: 372.34156723584204\n",
            "9: [[-0.74413014]\n",
            " [ 0.14729873]]\n",
            "Estimated Parameters (Weights): [[-0.74413014]\n",
            " [ 0.14729873]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the initial values\n",
        "debugstart = torch.tensor([1, 2, 3], dtype = torch.float64, requires_grad = True)\n",
        "x0 = torch.tensor([1, 1/2, 1/3], dtype = torch.float64, requires_grad = True)\n",
        "y1 = torch.tensor([1], dtype = torch.float64, requires_grad = True)\n",
        "\n",
        "# Calculate xbhat\n",
        "xbhat = torch.mm(x0.unsqueeze(0), debugstart.unsqueeze(1))\n",
        "xbhat.retain_grad()\n",
        "\n",
        "# Calculate yhat\n",
        "yhat = torch.sigmoid(xbhat)\n",
        "yhat.retain_grad()\n",
        "\n",
        "# Calculate the loss\n",
        "loss = -torch.sum(y1 * torch.log(yhat) + (1 - y1) * torch.log(1 - yhat))\n",
        "loss.backward()\n",
        "\n",
        "# Print the tensors and their gradients\n",
        "print(\"y:\", y1)\n",
        "print(\"x:\", x0)\n",
        "print(\"xbhat:\", xbhat)\n",
        "print(\"Gradients of xbhat (dloss/dp):\", xbhat.grad)\n",
        "print(\"yhat:\", yhat)\n",
        "print(\"Gradients of yhat (dloss/dyhat):\", yhat.grad)\n",
        "print(\"Loss:\", loss.item())"
      ],
      "metadata": {
        "id": "5yVLjqRvyYOw",
        "outputId": "2c15ce53-0e28-4c17-f431-5474b724b8ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y: tensor([1.], dtype=torch.float64, requires_grad=True)\n",
            "x: tensor([1.0000, 0.5000, 0.3333], dtype=torch.float64, requires_grad=True)\n",
            "xbhat: tensor([[3.]], dtype=torch.float64, grad_fn=<MmBackward0>)\n",
            "Gradients of xbhat (dloss/dp): tensor([[-0.0474]], dtype=torch.float64)\n",
            "yhat: tensor([[0.9526]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n",
            "Gradients of yhat (dloss/dyhat): tensor([[-1.0498]], dtype=torch.float64)\n",
            "Loss: 0.04858735157374191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEURAL NETWORK LOGIT"
      ],
      "metadata": {
        "id": "9XOZy2a5Ojma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "cBhQa6x3OlwS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple logistic regression model\n",
        "class LogitModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogitModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # Single output unit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.linear.weight.dtype)  # Cast input to the same data type as the model's parameters\n",
        "        x = self.linear(x)\n",
        "        x = torch.sigmoid(x)  # Apply sigmoid activation for binary classification\n",
        "        return x\n",
        "\n",
        "# Assuming you have 'train' DataFrame with 'balance' and 'income' columns\n",
        "x = torch.tensor(x_train, dtype = torch.float64)\n",
        "x.requires_grad = True\n",
        "\n",
        "y = torch.tensor(y_train, dtype = torch.float64)\n",
        "y.requires_grad = True\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "beta = torch.randn(x.shape[1], 1, dtype = torch.float64)\n",
        "beta.requires_grad = True\n",
        "\n",
        "# Define model input dimension based on the number of features in your data\n",
        "input_dim = x.shape[1]\n",
        "\n",
        "# Create an instance of the LogitModel\n",
        "model = LogitModel(input_dim)\n",
        "\n",
        "# Define a binary cross-entropy loss function\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Set the number of training epochs\n",
        "epochs = 5000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(x)\n",
        "\n",
        "    outputs = outputs.float()\n",
        "    y = y.float()\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_function(outputs, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model's parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training progress\n",
        "    if epoch % (epochs / 10) == 0:\n",
        "        print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# View the learned parameters\n",
        "print(\"Learned parameters:\")\n",
        "print(f\"{model.linear.weight}\") # these are the parameters of interest\n",
        "print(f\"{model.linear.bias}\")"
      ],
      "metadata": {
        "id": "Rs6icLu8ijPF",
        "outputId": "83c3522d-d134-4b0f-c556-82a4b22da5c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/5000], Loss: 0.6974\n",
            "Epoch [500/5000], Loss: 0.5904\n",
            "Epoch [1000/5000], Loss: 0.5904\n",
            "Epoch [1500/5000], Loss: 0.5904\n",
            "Epoch [2000/5000], Loss: 0.5904\n",
            "Epoch [2500/5000], Loss: 0.5904\n",
            "Epoch [3000/5000], Loss: 0.5904\n",
            "Epoch [3500/5000], Loss: 0.5904\n",
            "Epoch [4000/5000], Loss: 0.5904\n",
            "Epoch [4500/5000], Loss: 0.5904\n",
            "Learned parameters:\n",
            "Parameter containing:\n",
            "tensor([[-0.2359,  1.0371]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.3728], requires_grad=True)\n"
          ]
        }
      ]
    }
  ]
}